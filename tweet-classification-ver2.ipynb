{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries that I need to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\r\n",
      "  Downloading bert-for-tf2-0.13.5.tar.gz (40 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 40 kB 1.7 MB/s \r\n",
      "\u001b[?25hCollecting py-params>=0.7.3\r\n",
      "  Downloading py-params-0.9.6.tar.gz (6.6 kB)\r\n",
      "Collecting params-flow>=0.7.1\r\n",
      "  Downloading params-flow-0.7.4.tar.gz (19 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.7.1->bert-for-tf2) (1.18.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.7.1->bert-for-tf2) (4.42.0)\r\n",
      "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\r\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.13.5-py3-none-any.whl size=29946 sha256=8ac17447396905ca12ef90886d34726ecc6a03b781c1db2486c809e765088a40\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/00/4a/e3ae98003e155850dee523617ced7794e6e3da07d27e3802a0\r\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for py-params: filename=py_params-0.9.6-py3-none-any.whl size=7088 sha256=05806a4c7d742384c0756263ce171c43c30a183fbb409bd31570afb2f4ee005b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/4d/5a/7fbbdfb87bf3da12265e0d79dc19ce143652a09e3696189eb8\r\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.7.4-py3-none-any.whl size=16194 sha256=9e8bf7a360f2a6248771e391651acd86a0ec0ca9c2a2c026a2f8118c5a3289e7\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/f4/90/ca/a5d500c088762773489e532acd0268226cf2597f211928f38a\r\n",
      "Successfully built bert-for-tf2 py-params params-flow\r\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\r\n",
      "Successfully installed bert-for-tf2-0.13.5 params-flow-0.7.4 py-params-0.9.6\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.85)\r\n",
      "Collecting pycontractions\r\n",
      "  Downloading pycontractions-2.0.1-py3-none-any.whl (9.6 kB)\r\n",
      "Requirement already satisfied: gensim>=2.0 in /opt/conda/lib/python3.6/site-packages (from pycontractions) (3.8.1)\r\n",
      "Collecting language-check>=1.0\r\n",
      "  Downloading language-check-1.1.tar.gz (33 kB)\r\n",
      "Requirement already satisfied: pyemd>=0.4.4 in /opt/conda/lib/python3.6/site-packages (from pycontractions) (0.5.1)\r\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.14.0)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.9.0)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.18.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.22.0)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.12.13)\r\n",
      "Requirement already satisfied: boto>=2.32 in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.49.0)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2019.11.28)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.24.3)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8)\r\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.3.3)\r\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.13 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.15.13)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.9.5)\r\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.13->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.15.2)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.13->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8.0)\r\n",
      "Building wheels for collected packages: language-check\r\n",
      "  Building wheel for language-check (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for language-check: filename=language_check-1.1-py3-none-any.whl size=90190895 sha256=13fa39f1fc1aae22e58410bbfd15839b49aad60ec40a26cd38fe39fd18782050\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/fe/32/3b83a67c4f1182f7f6aa134c1d04cdcd893072bdadb4f5a64c\r\n",
      "Successfully built language-check\r\n",
      "Installing collected packages: language-check, pycontractions\r\n",
      "Successfully installed language-check-1.1 pycontractions-2.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece\n",
    "# !pip install pdpipe \n",
    "# !pip install symspellpy\n",
    "!pip install pycontractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "\n",
    "\n",
    "#data pipeline \n",
    "# import pdpipe as pdp\n",
    "\n",
    "#Other Preprocessing\n",
    "import re\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.pipeline import FeatureUnion\n",
    "import string\n",
    "# !pip install symspellpy\n",
    "import pkg_resources\n",
    "# from symspellpy.symspellpy import SymSpell\n",
    "# from symspellpy import SymSpell, Verbosity\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Contraction Import\n",
    "from pycontractions import Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the text and storing it... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping done for Abbreviations and then preprocessed\n",
    "Version Edit: Anit G.\n",
    "* Removed Abusive words\n",
    "* Removed empty slangs\n",
    "* Removed slangs such as \"?- it means some body is asking question\"\n",
    "* Removed slangs with length more than 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests, json, csv\n",
    "# import re\n",
    "\n",
    "# def remove_wrong_abb(key,value):\n",
    "#     if(re.search('it means',value)):\n",
    "#         return False;\n",
    "#     elif(re.search('\\*',value)):\n",
    "#         return False\n",
    "#     elif(value==\"\"):\n",
    "#         return False\n",
    "#     elif(len(key)>7):\n",
    "#         return False\n",
    "#     else :\n",
    "#         return True\n",
    "    \n",
    "# resp = requests.get(\"http://www.netlingo.com/acronyms.php\")\n",
    "# soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "# slangdict= {}\n",
    "# key=\"\"\n",
    "# value=\"\"\n",
    "# for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "#     for li in div.findAll('li'):\n",
    "#         for a in li.findAll('a'):\n",
    "#             key =a.text\n",
    "#             value = li.text.split(key)[1]\n",
    "#             if(remove_wrong_abb(key,value)):\n",
    "#                 if(re.search('\\ -or- ',value)):\n",
    "#                     pos=re.search('\\ -or- ',value).start()\n",
    "#                     slangdict[key]=value[:pos]\n",
    "#                 else:\n",
    "#                     slangdict[key]=value\n",
    "\n",
    "                    \n",
    "# # with open('myslang.json', 'w') as f:\n",
    "# #     json.dump(slangdict, f, indent=2)\n",
    "\n",
    "# w = csv.writer(open(\"myslang.csv\", \"w\"))\n",
    "# for key, val in slangdict.items():\n",
    "#     w.writerow([key.lower(), val.lower()])\n",
    "    \n",
    "# reader = csv.reader(open('myslang.csv', 'r'))\n",
    "# abbreviations = {}\n",
    "# for row in reader:\n",
    "#     k, v = row\n",
    "#     abbreviations[k] = v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Preprocessing Configuration and Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SymSpell - Unbeatable Spell Checker\n",
    "\n",
    "**Configuration Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Version Edit: Sonam D.\n",
    "# sym_spell_4space = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
    "# dictionary_path = pkg_resources.resource_filename(\n",
    "#     \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# sym_spell_4space.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "\n",
    "# sym_spell_misspelled = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "# bigram_path = pkg_resources.resource_filename(\n",
    "#     \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "# sym_spell_misspelled.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "# sym_spell_misspelled.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Trained for Contraction handling - pycontractions\n",
    "\n",
    "**Configuration Setup**\n",
    "[](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====---------------------------------------------] 11.9% 46.0/387.1MB downloaded"
     ]
    }
   ],
   "source": [
    "cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "cont.load_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing  Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Version Edit: Sonam D.\n",
    "def to_lower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+|pic.twitter.com\\S+')\n",
    "    return url.sub(r' ',text)\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r' ',text)\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', text)\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_special_ucchar(text):\n",
    "    text = re.sub('&.*?;', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_numbers(text):\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(r'@\\w*', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_punctuations(text):\n",
    "    text = re.sub(r'([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_square_bracket(text):\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_angular_bracket(text):\n",
    "    text = re.sub('\\<.*?\\>+', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_newline(text):\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_words_with_numbers(text):\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    return text\n",
    "    \n",
    "#Version Edit: Susanth D.\n",
    "def hashtag_to_words(text):\n",
    "    text = re.sub(r'##', '#', text)\n",
    "    hash_pattern = re.compile(r\"#\\w*\")\n",
    "    hashtag_list = re.findall(r\"#\\w+\",text)\n",
    "    for hashtag in hashtag_list:\n",
    "        hashtag = re.sub(r'#', '', hashtag)\n",
    "        text = re.sub(hashtag, sym_spell_4space.word_segmentation(hashtag).corrected_string, text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "#     text = re.sub(r'# ', '', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_stopwords(text):\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        if token not in stopwords.words('english'):\n",
    "            textop = textop + token + ' '\n",
    "    return textop\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def correct_misspelled_with_context(text):\n",
    "    suggestions = sym_spell_misspelled.lookup_compound(text, max_edit_distance=2)\n",
    "    text = str(suggestions[0])\n",
    "    text = re.sub(r', \\d', ' ', text)\n",
    "#     text = remove_numbers(text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def stemming_text(text):\n",
    "    stemmer= PorterStemmer()\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        textop = textop + stemmer.stem(token) + ' '\n",
    "    return textop\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def lemmatization(text):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        textop = textop + lemmatizer.lemmatize(token) + ' '\n",
    "    return textop\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def convert_abbrev(word):\n",
    "    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def convert_abbrev_in_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [convert_abbrev(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "# def handle_contractions(text):\n",
    "#     text = re.sub(r\"’\", \"'\", text)\n",
    "#     for word in text.split():\n",
    "#         if word.lower() in contractions:\n",
    "#             text = text.replace(word, contractions[word.lower()])\n",
    "#     return(text)\n",
    "\n",
    "#Version Edit: Saurabh M.\n",
    "def removeRepeated(tweet):\n",
    "    prev = ''\n",
    "    tweet_new = ''\n",
    "    for c in tweet:\n",
    "        caps = False\n",
    "        if c.isdigit():\n",
    "            tweet_new += c\n",
    "            continue\n",
    "        if c.isalpha() == True:\n",
    "            if ord(c) >= 65 and ord(c)<=90:\n",
    "                caps = True\n",
    "            c = c.lower()\n",
    "            if c == prev:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "                prev = c\n",
    "            if count >= 3:\n",
    "                continue\n",
    "            if caps == True:\n",
    "                tweet_new += c.upper()\n",
    "            else:\n",
    "                tweet_new += c\n",
    "        else:\n",
    "            tweet_new += c\n",
    "    return tweet_new\n",
    "\n",
    "\n",
    "def Expand_Contractions(text):\n",
    "    return list(cont.expand_texts([text]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Data Pipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(text):\n",
    "    new_text = text.apply(lambda x : list(x)).explode()\n",
    "    return new_text.unique().shape[0]\n",
    "\n",
    "def count_words(text):\n",
    "    new_text = text.apply(lambda x : x.split(' ')).explode()\n",
    "    return new_text.unique().shape[0]\n",
    "\n",
    "def preprocess_pipeline(steps, col, df):\n",
    "    new_col = df[col]\n",
    "    char_count_before = 0\n",
    "    word_count_before = 0\n",
    "    char_count_after = 0\n",
    "    word_count_after = 0\n",
    "    for each_step in steps:\n",
    "        char_count_before = count_chars(new_col)\n",
    "        word_count_before = count_words(new_col)\n",
    "        new_col = new_col.apply(each_step)\n",
    "        char_count_after = count_chars(new_col)\n",
    "        word_count_after = count_words(new_col)\n",
    "        print(\"Preprocessing step: \",each_step.__name__)\n",
    "        print(\"Char Count ---> Before: %d | After: %d\"%(char_count_before, char_count_after))\n",
    "        print(\"Word Count ---> Before: %d | After: %d\"%(word_count_before, word_count_after))\n",
    "    \n",
    "    return new_col\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing step:  remove_newline\n",
      "Char Count ---> Before: 122 | After: 121\n",
      "Word Count ---> Before: 32017 | After: 31925\n",
      "Preprocessing step:  remove_URL\n",
      "Char Count ---> Before: 121 | After: 121\n",
      "Word Count ---> Before: 31925 | After: 27333\n",
      "Preprocessing step:  remove_html\n",
      "Char Count ---> Before: 121 | After: 121\n",
      "Word Count ---> Before: 27333 | After: 27333\n",
      "Preprocessing step:  remove_emoji\n",
      "Char Count ---> Before: 121 | After: 121\n",
      "Word Count ---> Before: 27333 | After: 27333\n",
      "Preprocessing step:  remove_special_ucchar\n",
      "Char Count ---> Before: 121 | After: 120\n",
      "Word Count ---> Before: 27333 | After: 27300\n",
      "Preprocessing step:  remove_mentions\n",
      "Char Count ---> Before: 120 | After: 119\n",
      "Word Count ---> Before: 27300 | After: 24973\n",
      "Preprocessing step:  Expand_Contractions\n",
      "Char Count ---> Before: 119 | After: 119\n",
      "Word Count ---> Before: 24973 | After: 24801\n",
      "Preprocessing step:  remove_punctuations\n",
      "Char Count ---> Before: 119 | After: 53\n",
      "Word Count ---> Before: 24801 | After: 18130\n",
      "Preprocessing step:  removeRepeated\n",
      "Char Count ---> Before: 53 | After: 53\n",
      "Word Count ---> Before: 18130 | After: 18134\n",
      "Preprocessing step:  lemmatization\n",
      "Char Count ---> Before: 53 | After: 53\n",
      "Word Count ---> Before: 18134 | After: 17217\n",
      "\n",
      "Preprocessing step:  remove_newline\n",
      "Char Count ---> Before: 118 | After: 117\n",
      "Word Count ---> Before: 17426 | After: 17448\n",
      "Preprocessing step:  remove_URL\n",
      "Char Count ---> Before: 117 | After: 117\n",
      "Word Count ---> Before: 17448 | After: 15417\n",
      "Preprocessing step:  remove_html\n",
      "Char Count ---> Before: 117 | After: 117\n",
      "Word Count ---> Before: 15417 | After: 15417\n",
      "Preprocessing step:  remove_emoji\n",
      "Char Count ---> Before: 117 | After: 117\n",
      "Word Count ---> Before: 15417 | After: 15417\n",
      "Preprocessing step:  remove_special_ucchar\n",
      "Char Count ---> Before: 117 | After: 116\n",
      "Word Count ---> Before: 15417 | After: 15406\n",
      "Preprocessing step:  remove_mentions\n",
      "Char Count ---> Before: 116 | After: 115\n",
      "Word Count ---> Before: 15406 | After: 14267\n",
      "Preprocessing step:  Expand_Contractions\n",
      "Char Count ---> Before: 115 | After: 115\n",
      "Word Count ---> Before: 14267 | After: 14141\n",
      "Preprocessing step:  remove_punctuations\n",
      "Char Count ---> Before: 115 | After: 53\n",
      "Word Count ---> Before: 14141 | After: 11078\n",
      "Preprocessing step:  removeRepeated\n",
      "Char Count ---> Before: 53 | After: 53\n",
      "Word Count ---> Before: 11078 | After: 11095\n",
      "Preprocessing step:  lemmatization\n",
      "Char Count ---> Before: 53 | After: 53\n",
      "Word Count ---> Before: 11095 | After: 10584\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All resident asked to shelter in place are bei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order in Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska a sm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                     processed_text  \n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...  \n",
       "1       1             Forest fire near La Ronge Sask Canada   \n",
       "2       1  All resident asked to shelter in place are bei...  \n",
       "3       1  people receive wildfire evacuation order in Ca...  \n",
       "4       1  Just got sent this photo from Ruby Alaska a sm...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "\n",
    "\n",
    "pipeline = []\n",
    "# pipeline.append(to_lower)\n",
    "pipeline.append(remove_newline)\n",
    "pipeline.append(remove_URL)\n",
    "pipeline.append(remove_html)\n",
    "pipeline.append(remove_emoji)\n",
    "pipeline.append(remove_special_ucchar)\n",
    "# pipeline.append(hashtag_to_words)\n",
    "# pipeline.append(remove_words_with_numbers)\n",
    "# pipeline.append(remove_numbers)\n",
    "pipeline.append(remove_mentions)\n",
    "# pipeline.append(remove_square_bracket)\n",
    "# pipeline.append(remove_angular_bracket)\n",
    "pipeline.append(Expand_Contractions)\n",
    "pipeline.append(remove_punctuations)\n",
    "pipeline.append(removeRepeated)\n",
    "# pipeline.append(convert_abbrev_in_text)\n",
    "# pipeline.append(remove_stopwords)\n",
    "# pipeline.append(correct_misspelled_with_context)\n",
    "# pipeline.append(remove_numbers)\n",
    "# pipeline.append(remove_stopwords)\n",
    "# # pipeline.append(stemming_text)\n",
    "pipeline.append(lemmatization)\n",
    "\n",
    "train['processed_text'] = preprocess_pipeline(pipeline, 'text', train)\n",
    "print()\n",
    "test['processed_text'] = preprocess_pipeline(pipeline, 'text', test)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>208</td>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>Eagle Pass, Texas</td>\n",
       "      <td>A Cessna airplane accident in Ocampo Coahuila ...</td>\n",
       "      <td>1</td>\n",
       "      <td>A Cessna airplane accident in Ocampo Coahuila ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>688</td>\n",
       "      <td>attack</td>\n",
       "      <td>Dayton, Ohio</td>\n",
       "      <td>A Dayton-area org tells me it was hit by a cyb...</td>\n",
       "      <td>1</td>\n",
       "      <td>A Dayton area org tell me it wa hit by a cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7320</th>\n",
       "      <td>10479</td>\n",
       "      <td>wild%20fires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@EnzasBargains A5 Donated some fruit snacks &amp;a...</td>\n",
       "      <td>1</td>\n",
       "      <td>A Donated some fruit snack handi wipe to our f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7045</th>\n",
       "      <td>10092</td>\n",
       "      <td>typhoon</td>\n",
       "      <td>india</td>\n",
       "      <td>A GPM satellite 'bullseye' in Typhoon Soudelor...</td>\n",
       "      <td>1</td>\n",
       "      <td>A GPM satellite bullseye in Typhoon Soudelor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6858</th>\n",
       "      <td>9830</td>\n",
       "      <td>trauma</td>\n",
       "      <td>www.aprylpooley.com</td>\n",
       "      <td>A1: I started writing when I couldn't talk abo...</td>\n",
       "      <td>0</td>\n",
       "      <td>A I started writing when I could not talk abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>1949</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@MoFanon ?? your last retweet you would think ...</td>\n",
       "      <td>0</td>\n",
       "      <td>your last retweet you would think the lion sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>198</td>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>Salt Lake City, Utah</td>\n",
       "      <td>@crobscarla your lifetime odds of dying from a...</td>\n",
       "      <td>0</td>\n",
       "      <td>your lifetime odds of dying from an airplane a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <td>4411</td>\n",
       "      <td>electrocute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@i_electroCute your turn ??</td>\n",
       "      <td>0</td>\n",
       "      <td>your turn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6382</th>\n",
       "      <td>9120</td>\n",
       "      <td>suicide%20bomb</td>\n",
       "      <td>lagos. Unilag</td>\n",
       "      <td>16yr old PKK suicide bomber who detonated bomb...</td>\n",
       "      <td>1</td>\n",
       "      <td>yr old PKK suicide bomber who detonated bomb i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6394</th>\n",
       "      <td>9139</td>\n",
       "      <td>suicide%20bomb</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>16yr old boy pic PKK suicide bomber who detona...</td>\n",
       "      <td>1</td>\n",
       "      <td>yr old boy pic PKK suicide bomber who detonate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6826 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id              keyword              location  \\\n",
       "144     208  airplane%20accident     Eagle Pass, Texas   \n",
       "478     688               attack          Dayton, Ohio   \n",
       "7320  10479         wild%20fires                   NaN   \n",
       "7045  10092              typhoon                 india   \n",
       "6858   9830               trauma   www.aprylpooley.com   \n",
       "...     ...                  ...                   ...   \n",
       "1348   1949  burning%20buildings                   NaN   \n",
       "138     198  airplane%20accident  Salt Lake City, Utah   \n",
       "3075   4411          electrocute                   NaN   \n",
       "6382   9120       suicide%20bomb         lagos. Unilag   \n",
       "6394   9139       suicide%20bomb               Nigeria   \n",
       "\n",
       "                                                   text  target  \\\n",
       "144   A Cessna airplane accident in Ocampo Coahuila ...       1   \n",
       "478   A Dayton-area org tells me it was hit by a cyb...       1   \n",
       "7320  @EnzasBargains A5 Donated some fruit snacks &a...       1   \n",
       "7045  A GPM satellite 'bullseye' in Typhoon Soudelor...       1   \n",
       "6858  A1: I started writing when I couldn't talk abo...       0   \n",
       "...                                                 ...     ...   \n",
       "1348  @MoFanon ?? your last retweet you would think ...       0   \n",
       "138   @crobscarla your lifetime odds of dying from a...       0   \n",
       "3075                        @i_electroCute your turn ??       0   \n",
       "6382  16yr old PKK suicide bomber who detonated bomb...       1   \n",
       "6394  16yr old boy pic PKK suicide bomber who detona...       1   \n",
       "\n",
       "                                         processed_text  \n",
       "144   A Cessna airplane accident in Ocampo Coahuila ...  \n",
       "478   A Dayton area org tell me it wa hit by a cyber...  \n",
       "7320  A Donated some fruit snack handi wipe to our f...  \n",
       "7045      A GPM satellite bullseye in Typhoon Soudelor   \n",
       "6858  A I started writing when I could not talk abou...  \n",
       "...                                                 ...  \n",
       "1348  your last retweet you would think the lion sav...  \n",
       "138   your lifetime odds of dying from an airplane a...  \n",
       "3075                                         your turn   \n",
       "6382  yr old PKK suicide bomber who detonated bomb i...  \n",
       "6394  yr old boy pic PKK suicide bomber who detonate...  \n",
       "\n",
       "[6826 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, idx = np.unique(train['processed_text'], return_index=True)\n",
    "train.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "when you are taking a shower and someone flush the toilet and you have second to GTFO or you get burned \n"
     ]
    }
   ],
   "source": [
    "tweet_len = train['processed_text'].apply(len)\n",
    "print(tweet_len.max())\n",
    "print(train['processed_text'].iloc[1270])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    }
   ],
   "source": [
    "tweet_len = test['processed_text'].apply(len)\n",
    "print(tweet_len.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline=pdp.ApplyByCols('text',to_lower,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_newline,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_URL,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_html,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_emoji,'text')\n",
    "# # pipeline+=pdp.ApplyByCols('text',hashtag_to_words,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_words_with_numbers,'text')\n",
    "# # pipeline+=pdp.ApplyByCols('text',remove_numbers,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_mentions,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_square_bracket,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_angular_bracket,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',Expand_Contractions,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_punctuations,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',removeRepeated,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',convert_abbrev_in_text,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',correct_misspelled_with_context,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_numbers,'text')\n",
    "# # pipeline+=pdp.ApplyByCols('text',remove_stopwords,'text')\n",
    "# # pipeline+=pdp.ApplyByCols('text',stemming_text,'text')\n",
    "# # pipeline+=pdp.ApplyByCols('text',lemmatization,'text')\n",
    "\n",
    "\n",
    "\n",
    "# # df=pd.concat([train,test],sort=False)\n",
    "# train=pipeline(train)\n",
    "# test=pipeline(test)\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# import nltk\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer5 = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "# # appling tokenizer5\n",
    "# train['text_tokens'] = train['text_processed'].apply(lambda x: tokenizer5.tokenize(x))\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, \n",
    "#                                                   train.target,\n",
    "#                                                   random_state=42, \n",
    "#                                                   test_size=0.2, shuffle=True)\n",
    "# print(xtrain.shape)\n",
    "# print(xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Appling CountVectorizer()\n",
    "# count_vectorizer = CountVectorizer()\n",
    "# xtrain_vectors = count_vectorizer.fit_transform(xtrain)\n",
    "# xvalid_vectors = count_vectorizer.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fitting a simple xgboost on CountVec\n",
    "# clf = xgb.XGBClassifier(max_depth=10, n_estimators=300, colsample_bytree=0.3, \n",
    "#                         subsample=0.5, nthread=10, learning_rate=0.1, gamma=0.7, reg_alpha=0.01, reg_lambda=0.01)\n",
    "# clf.fit(xtrain_vectors, ytrain)\n",
    "\n",
    "# predictions = clf.predict(xvalid_vectors)\n",
    "# print('XGBClassifier on CountVectorizer')\n",
    "# print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Appling TFIDF\n",
    "# tfidf = TfidfVectorizer(ngram_range=(1, 2), norm='l2')\n",
    "# xtrain_tfidf = tfidf.fit_transform(xtrain)\n",
    "# xvalid_tfidf = tfidf.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fitting a simple xgboost on TFIDF\n",
    "# clf = xgb.XGBClassifier(max_depth=100, n_estimators=300, colsample_bytree=0.3, \n",
    "#                         subsample=0.5, nthread=10, learning_rate=0.1)\n",
    "# clf.fit(xtrain_tfidf.tocsc(), ytrain)\n",
    "# predictions = clf.predict(xvalid_tfidf.tocsc())\n",
    "\n",
    "# print('XGBClassifier on TFIDF')\n",
    "# print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model [Implementation 1]-  Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BERT Methods Predefined\"\"\"\n",
    "def bert_encode(texts, tokenizer, max_len=150):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7613 rows and 6 columns in train\n",
      "There are 3263 rows and 5 columns in train\n"
     ]
    }
   ],
   "source": [
    "print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\n",
    "print('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Implementation\n",
    "\n",
    "### Downloading a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT from the Tensorflow Hub\n",
    "\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "# module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "#https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\n",
    "#https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/1\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenising Tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_train_input = bert_encode(train.processed_text.values, tokenizer, max_len=160)\n",
    "# test_input = bert_encode(test.processed_text.values, tokenizer, max_len=160)\n",
    "# full_train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, train_labels, val_labels = train_test_split(train.processed_text.values, train.target.values, test_size=0.2)\n",
    "train_input = bert_encode(train_data, tokenizer, max_len=150)\n",
    "val_input = bert_encode(val_data, tokenizer, max_len=150)\n",
    "test_input = bert_encode(test.processed_text.values, tokenizer, max_len=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras import regularizers\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=9e-6\n",
    "decay=1e-8\n",
    "Dropout_num=0\n",
    "max_len=150\n",
    "# lr_schedule = [5e-8,1e-8,5e-9,1e-9]\n",
    "# lr_schedule = [1e-6, 5e-7, 1e-7, 5e-8, 1e-8, 1e-9]\n",
    "lr_schedule = [9e-7,1e-8,9e-8,5e-8,7e-9,1e-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "_, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "clf_output = sequence_output[:, 0, :]\n",
    "\n",
    "## Type1\n",
    "# out = Dense(100, activation='relu', activity_regularizer=regularizers.l2(9e-5))(clf_output)\n",
    "# out = Dense(100, activation='relu', activity_regularizer=regularizers.l2(9e-5))(out)\n",
    "# out = Dense(100, activation='relu')(out)\n",
    "\n",
    "## Type2\n",
    "out = Dense(1024, activation='relu', activity_regularizer=regularizers.l2(9e-5))(clf_output)\n",
    "\n",
    "## Type3\n",
    "# out = clf_output\n",
    "\n",
    "out = Dense(1, activation='sigmoid')(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         1049600     tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            1025        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 336,192,514\n",
      "Trainable params: 336,192,513\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sBERT = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "# sBERT.compile(SGD(lr=learning_rate, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "sBERT.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "sBERT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "6090/6090 [==============================] - 437s 72ms/sample - loss: 0.4296 - accuracy: 0.8184 - val_loss: 0.4072 - val_accuracy: 0.8260\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "                               min_delta=0,\n",
    "                               patience=2,\n",
    "                               verbose=1, \n",
    "                               mode='auto')\n",
    "\n",
    "checkpoint1 = ModelCheckpoint('best_accuracy.h5',\n",
    "                             monitor='val_accuracy',\n",
    "                             save_best_only=True)\n",
    "# checkpoint2 = ModelCheckpoint('best_loss.h5',\n",
    "#                              monitor='val_loss',\n",
    "#                              save_best_only=True)\n",
    "\n",
    "\n",
    "train_history = sBERT.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_data=(val_input, val_labels),\n",
    "    epochs = 1,\n",
    "    callbacks=[checkpoint1],\n",
    "    batch_size = 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.set_value(sBERT.optimizer.lr, 1e-6)\n",
    "# idx = np.random.permutation(train_input[0].shape[0])\n",
    "# input1, input2, input3 = train_input\n",
    "# input1 =input1[idx]\n",
    "# input2 =input2[idx]\n",
    "# input3 =input3[idx]\n",
    "# train_input = input1, input2, input3\n",
    "# train_labels = train_labels[idx]\n",
    "# train_history = sBERT.fit(\n",
    "#     train_input, train_labels,\n",
    "#     validation_data = (val_input, val_labels),\n",
    "#     epochs = 1, # recomended 3-5 epochs\n",
    "#     callbacks=[checkpoint1, checkpoint2],\n",
    "#     batch_size = 48 # recomended 8-16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "3200/6090 [==============>...............] - ETA: 3:38 - loss: 0.2597 - accuracy: 0.9019 - val_loss: 0.4189 - val_accuracy: 0.8286Train on 6090 samples, validate on 1523 samples\n",
      "3200/6090 [==============>...............] - ETA: 3:05 - loss: 0.2471 - accuracy: 0.9066 - val_loss: 0.4188 - val_accuracy: 0.8286Train on 6090 samples, validate on 1523 samples\n",
      "3200/6090 [==============>...............] - ETA: 3:05 - loss: 0.2379 - accuracy: 0.9119 - val_loss: 0.4201 - val_accuracy: 0.8273Train on 6090 samples, validate on 1523 samples\n",
      "3200/6090 [==============>...............] - ETA: 3:05 - loss: 0.2452 - accuracy: 0.9056 - val_loss: 0.4206 - val_accuracy: 0.8286Train on 6090 samples, validate on 1523 samples\n",
      "3200/6090 [==============>...............] - ETA: 3:37 - loss: 0.2434 - accuracy: 0.9097 - val_loss: 0.4207 - val_accuracy: 0.8293"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    K.set_value(sBERT.optimizer.lr, lr_schedule[i])\n",
    "    idx = np.random.permutation(train_input[0].shape[0])\n",
    "    input1, input2, input3 = train_input\n",
    "    input1 = input1[idx]\n",
    "    input2 = input2[idx]\n",
    "    input3 = input3[idx]\n",
    "    train_input = input1, input2, input3\n",
    "    train_labels = train_labels[idx]\n",
    "    train_history = sBERT.fit(\n",
    "        train_input, train_labels,\n",
    "        validation_data = (val_input, val_labels),\n",
    "        epochs = 1, \n",
    "        steps_per_epoch = 200,\n",
    "        callbacks = [checkpoint1],\n",
    "        batch_size = 16 \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_history = sBERT.fit(\n",
    "#     train_input, train_labels,\n",
    "#     validation_split = 0.2,\n",
    "#     epochs = 1, # recomended 3-5 epochs\n",
    "# #     steps_per_epoch=320,\n",
    "#     callbacks=[checkpoint1],\n",
    "#     batch_size = 16 # recomended 8-16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Susanth's Full model\n",
    "test_pred = sBERT.predict(test_input)\n",
    "sub['target'] = test_pred.round().astype(int)\n",
    "sub.to_csv('submission full.csv', index=False)\n",
    "sub.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Susanth's checkpoint model\n",
    "sBERT.load_weights('best_accuracy.h5')\n",
    "test_pred = sBERT.predict(test_input)\n",
    "sub['target'] = test_pred.round().astype(int)\n",
    "sub.to_csv('submission checkpoint.csv', index=False)\n",
    "sub.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Susanth's checkpoint model\n",
    "# sBERT.load_weights('best_loss.h5')\n",
    "# test_pred = sBERT.predict(test_input)\n",
    "# sub['target'] = test_pred.round().astype(int)\n",
    "# sub.to_csv('submission checkpoint best loss.csv', index=False)\n",
    "# sub.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Parameters Tried and Respective Accuracies for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Tune Parameter 1 ##################\n",
    "# Dropout_num = 0\n",
    "# learning_rate = 1e-7\n",
    "# valid = 0.3\n",
    "# epochs_num = 10\n",
    "# batch_size_num = 32\n",
    "# Dimensions = 768\n",
    "# Dense Layer = 1(Final)\n",
    "# Val_accuracy : 79.9\n",
    "# Test_accuracy : 90.31\n",
    "######################################################\n",
    "################## Tune Parameter 2 ##################\n",
    "# Dropout_num = 0\n",
    "# learning_rate = 1e-7\n",
    "# valid = 0.3\n",
    "# epochs_num = 10\n",
    "# batch_size_num = 32\n",
    "# Dimensions = 768\n",
    "# Dense Layer = 1\n",
    "# Dense 1 = 768 (Relu)\n",
    "# Desne 2 = 1 (Softmax)\n",
    "# Val_accuracy : 80.7\n",
    "# Test_accuracy : 90.20\n",
    "######################################################\n",
    "################## Tune Parameter 3 ##################\n",
    "#Dropout_num = 0\n",
    "#learning_rate = 9e-6\n",
    "#valid = 0.1\n",
    "#epochs_num = 3\n",
    "#batch_size_num = 16\n",
    "#Val_accuracy : \n",
    "######################################################\n",
    "################## Tune Parameter 4 ##################\n",
    "#Dropout_num = 0\n",
    "#learning_rate = 5e-6\n",
    "#valid = 0.1\n",
    "#epochs_num = 3\n",
    "#batch_size_num = 16\n",
    "#Val_accuracy : 81.63\n",
    "######################################################\n",
    "################## Tune Parameter 5 ##################\n",
    "#Dropout_num = 0\n",
    "#learning_rate = 5e-6\n",
    "#valid = 0.1\n",
    "#epochs_num = 5\n",
    "#batch_size_num = 16\n",
    "#Val_accuracy : \n",
    "######################################################\n",
    "################## Tune Parameter 6 ##################\n",
    "#Dropout_num = 0\n",
    "#learning_rate = 9e-6\n",
    "#valid = 0.1\n",
    "#epochs_num = 5\n",
    "#batch_size_num = 16\n",
    "#Val_accuracy : \n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
