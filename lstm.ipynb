{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install bert-for-tf2\n!pip install sentencepiece\n!pip install pdpipe \n!pip install symspellpy\n!pip install pycontractions","execution_count":50,"outputs":[{"output_type":"stream","text":"Collecting bert-for-tf2\n  Downloading bert-for-tf2-0.13.5.tar.gz (40 kB)\n\u001b[K     |████████████████████████████████| 40 kB 1.5 MB/s eta 0:00:011\n\u001b[?25hCollecting py-params>=0.7.3\n  Downloading py-params-0.9.6.tar.gz (6.6 kB)\nCollecting params-flow>=0.7.1\n  Downloading params-flow-0.7.4.tar.gz (19 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.7.1->bert-for-tf2) (1.18.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.7.1->bert-for-tf2) (4.42.0)\nBuilding wheels for collected packages: bert-for-tf2, py-params, params-flow\n  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.13.5-py3-none-any.whl size=29946 sha256=016547466cf28ccfb2efb0ae3a20016295dc221e722b2eea9e58be95629850dd\n  Stored in directory: /root/.cache/pip/wheels/90/00/4a/e3ae98003e155850dee523617ced7794e6e3da07d27e3802a0\n  Building wheel for py-params (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for py-params: filename=py_params-0.9.6-py3-none-any.whl size=7088 sha256=69f613759fbf983026bc316fa5e339d8dd14a6924e6be95ecfbf6826de96f0d8\n  Stored in directory: /root/.cache/pip/wheels/d1/4d/5a/7fbbdfb87bf3da12265e0d79dc19ce143652a09e3696189eb8\n  Building wheel for params-flow (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.7.4-py3-none-any.whl size=16194 sha256=c1b967cb258bbbf9fedaa048dd5f3181c2e59a0b00f3d471235b6f8686205094\n  Stored in directory: /root/.cache/pip/wheels/f4/90/ca/a5d500c088762773489e532acd0268226cf2597f211928f38a\nSuccessfully built bert-for-tf2 py-params params-flow\nInstalling collected packages: py-params, params-flow, bert-for-tf2\nSuccessfully installed bert-for-tf2-0.13.5 params-flow-0.7.4 py-params-0.9.6\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.85)\nCollecting pdpipe\n  Downloading pdpipe-0.0.46-py3-none-any.whl (47 kB)\n\u001b[K     |████████████████████████████████| 47 kB 1.4 MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pdpipe) (4.42.0)\nRequirement already satisfied: sortedcontainers in /opt/conda/lib/python3.6/site-packages (from pdpipe) (2.1.0)\nCollecting skutil>=0.0.15\n  Downloading skutil-0.0.16-py2.py3-none-any.whl (18 kB)\nCollecting strct\n  Downloading strct-0.0.30-py2.py3-none-any.whl (16 kB)\nRequirement already satisfied: pandas>=0.18.0 in /opt/conda/lib/python3.6/site-packages (from pdpipe) (0.25.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from skutil>=0.0.15->pdpipe) (1.18.2)\nCollecting decore\n  Downloading decore-0.0.1.tar.gz (19 kB)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.18.0->pdpipe) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.18.0->pdpipe) (2019.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.18.0->pdpipe) (1.14.0)\nBuilding wheels for collected packages: decore\n  Building wheel for decore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for decore: filename=decore-0.0.1-py2.py3-none-any.whl size=4190 sha256=caa99aa6ecbd315d4f2aaa1aba719c48a0aa1a678e74beebd410fdbf638f7ca3\n  Stored in directory: /root/.cache/pip/wheels/3f/40/2a/59306076795b4b9b35a210a859053a479061021810c546ddbe\nSuccessfully built decore\nInstalling collected packages: decore, skutil, strct, pdpipe\nSuccessfully installed decore-0.0.1 pdpipe-0.0.46 skutil-0.0.16 strct-0.0.30\nCollecting symspellpy\n  Downloading symspellpy-6.5.2-py3-none-any.whl (2.6 MB)\n\u001b[K     |████████████████████████████████| 2.6 MB 2.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /opt/conda/lib/python3.6/site-packages (from symspellpy) (1.18.2)\nInstalling collected packages: symspellpy\nSuccessfully installed symspellpy-6.5.2\nCollecting pycontractions\n  Downloading pycontractions-2.0.1-py3-none-any.whl (9.6 kB)\nRequirement already satisfied: pyemd>=0.4.4 in /opt/conda/lib/python3.6/site-packages (from pycontractions) (0.5.1)\nCollecting language-check>=1.0\n  Downloading language-check-1.1.tar.gz (33 kB)\nRequirement already satisfied: gensim>=2.0 in /opt/conda/lib/python3.6/site-packages (from pycontractions) (3.8.1)\nRequirement already satisfied: numpy<2.0.0,>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from pyemd>=0.4.4->pycontractions) (1.18.2)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.10.0)\nRequirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.14.0)\nRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.4.1)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.12.32)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.22.0)\nRequirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.26.0)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.9.5)\nRequirement already satisfied: botocore<1.16.0,>=1.15.32 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.15.32)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.3.3)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.24.3)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2019.11.28)\nRequirement already satisfied: google-auth<2.0dev,>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.12.0)\nRequirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.3.0)\nRequirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.5.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.32->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8.1)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.32->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.15.2)\nRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (4.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.2.8)\nRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (46.1.3.post20200330)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (4.0.0)\nRequirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.16.0)\n","name":"stdout"},{"output_type":"stream","text":"Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.4.8)\nRequirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (3.11.3)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.51.0)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2019.3)\nBuilding wheels for collected packages: language-check\n  Building wheel for language-check (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for language-check: filename=language_check-1.1-py3-none-any.whl size=90190895 sha256=0850eb1007396c09dd39487fd2d9ac41305e9470ea5cf811b4b6e7ed4f5cbeaf\n  Stored in directory: /root/.cache/pip/wheels/ce/fe/32/3b83a67c4f1182f7f6aa134c1d04cdcd893072bdadb4f5a64c\nSuccessfully built language-check\nInstalling collected packages: language-check, pycontractions\nSuccessfully installed language-check-1.1 pycontractions-2.0.1\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport pickle\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n\n\n\n########################################################\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.models import Model\n\n\ntry:\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras import layers\nimport bert\n\n\n#data pipeline \nimport pdpipe as pdp\n\n#Other Preprocessing\nimport re\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nimport string\n# !pip install symspellpy\nimport pkg_resources\nfrom symspellpy.symspellpy import SymSpell\nfrom symspellpy import SymSpell, Verbosity\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport csv\nimport pandas as pd\n\nimport nltk\nnltk.download('punkt')\n\n#Contraction Import\nfrom pycontractions import Contractions","execution_count":103,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl\n/kaggle/input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain.head(3)","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n\n   target  \n0       1  \n1       1  \n2       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport requests, json, csv\nimport re\n\ndef remove_wrong_abb(key,value):\n    if(re.search('it means',value)):\n        return False;\n    elif(re.search('\\*',value)):\n        return False\n    elif(value==\"\"):\n        return False\n    elif(len(key)>7):\n        return False\n    else :\n        return True\n    \nresp = requests.get(\"http://www.netlingo.com/acronyms.php\")\nsoup = BeautifulSoup(resp.text, \"html.parser\")\nslangdict= {}\nkey=\"\"\nvalue=\"\"\nfor div in soup.findAll('div', attrs={'class':'list_box3'}):\n    for li in div.findAll('li'):\n        for a in li.findAll('a'):\n            key =a.text\n            value = li.text.split(key)[1]\n            if(remove_wrong_abb(key,value)):\n                if(re.search('\\ -or- ',value)):\n                    pos=re.search('\\ -or- ',value).start()\n                    slangdict[key]=value[:pos]\n                else:\n                    slangdict[key]=value\n\n                    \n# with open('myslang.json', 'w') as f:\n#     json.dump(slangdict, f, indent=2)\n\nw = csv.writer(open(\"myslang.csv\", \"w\"))\nfor key, val in slangdict.items():\n    w.writerow([key.lower(), val.lower()])\n    \nreader = csv.reader(open('myslang.csv', 'r'))\nabbreviations = {}\nfor row in reader:\n    k, v = row\n    abbreviations[k] = v","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Version Edit: Sonam D.\nsym_spell_4space = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\ndictionary_path = pkg_resources.resource_filename(\n    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\nsym_spell_4space.load_dictionary(dictionary_path, term_index=0, count_index=1)\n\n\nsym_spell_misspelled = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\nbigram_path = pkg_resources.resource_filename(\n    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\nsym_spell_misspelled.load_dictionary(dictionary_path, term_index=0, count_index=1)\nsym_spell_misspelled.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)","execution_count":54,"outputs":[{"output_type":"execute_result","execution_count":54,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont = Contractions(api_key=\"glove-twitter-100\")\ncont.load_models()","execution_count":55,"outputs":[{"output_type":"stream","text":"[==================================================] 100.0% 387.1/387.1MB downloaded\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Version Edit: Sonam D.\ndef to_lower(text):\n    text = text.lower()\n    return text\n\n#Version Edit: Anit G.\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r' ',text)\n\n#Version Edit: Anit G.\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r' ',text)\n\n#Version Edit: Anit G.\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r' ', text)\n\n#Version Edit: Sonam D.\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n#Version Edit: Sonam D.\ndef remove_numbers(text):\n    text = re.sub(r'\\d+', ' ', text)\n    return text\n\n#Version Edit: Sonam D.\ndef remove_mentions(text):\n    text = re.sub(r'@\\w*', ' ', text)\n    return text\n\n#Version Edit: Sonam D.\ndef remove_punctuations(text):\n    text = re.sub(r'([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_square_bracket(text):\n    text = re.sub('\\[.*?\\]', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_angular_bracket(text):\n    text = re.sub('<.*?>+', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_newline(text):\n    text = re.sub('\\n', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_words_with_numbers(text):\n    text = re.sub('\\w*\\d\\w*', ' ', text)\n    return text\n    \n#Version Edit: Sonam D.\ndef hashtag_to_words(text):\n    text = re.sub(r'##', '#', text)\n    hash_pattern = re.compile(r\"#\\w*\")\n    hashtag_list = re.findall(hash_pattern,text)\n    for hashtag in hashtag_list:\n        hashtag = re.sub(r'#', '', hashtag)\n        text = re.sub(hashtag, sym_spell_4space.word_segmentation(hashtag).corrected_string, text)\n    text = re.sub(r'#', '', text)\n    text = re.sub(r'# ', '', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_stopwords(text):\n    text_tokens=word_tokenize(text)\n    textop = ''\n    for token in text_tokens:\n        if token not in stopwords.words('english'):\n            textop = textop + token + ' '\n    return textop\n\n#Version Edit: Sonam D.\ndef correct_misspelled_with_context(text):\n    suggestions = sym_spell_misspelled.lookup_compound(text, max_edit_distance=2)\n    text = str(suggestions[0])\n    text = re.sub(r', \\d', ' ', text)\n#     text = remove_numbers(text)\n    return text\n\n#Version Edit: Sonam D.\ndef stemming_text(text):\n    stemmer= PorterStemmer()\n    text_tokens=word_tokenize(text)\n    textop = ''\n    for token in text_tokens:\n        textop = textop + stemmer.stem(token) + ' '\n    return textop\n\n#Version Edit: Sonam D.\ndef lemmatization(text):\n    lemmatizer=WordNetLemmatizer()\n    text_tokens=word_tokenize(text)\n    textop = ''\n    for token in text_tokens:\n        textop = textop + lemmatizer.lemmatize(token) + ' '\n    return textop\n\n#Version Edit: Anit G.\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\n#Version Edit: Anit G.\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text\n\n#Version Edit: Sonam D.\n# def handle_contractions(text):\n#     text = re.sub(r\"’\", \"'\", text)\n#     for word in text.split():\n#         if word.lower() in contractions:\n#             text = text.replace(word, contractions[word.lower()])\n#     return(text)\n\n#Version Edit: Saurabh M.\ndef removeRepeated(tweet):\n    prev = ''\n    tweet_new = ''\n    for c in tweet:\n        caps = False\n        if c.isdigit():\n            tweet_new += c\n            continue\n        if c.isalpha() == True:\n            if ord(c) >= 65 and ord(c)<=90:\n                caps = True\n            c = c.lower()\n            if c == prev:\n                count += 1\n            else:\n                count = 1\n                prev = c\n            if count >= 3:\n                continue\n            if caps == True:\n                tweet_new += c.upper()\n            else:\n                tweet_new += c\n        else:\n            tweet_new += c\n    return tweet_new\n\n\ndef Expand_Contractions(text):\n    return list(cont.expand_texts([text]))[0]","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline=pdp.ApplyByCols('text',to_lower,'text')\npipeline+=pdp.ApplyByCols('text',remove_newline,'text')\npipeline+=pdp.ApplyByCols('text',remove_URL,'text')\npipeline+=pdp.ApplyByCols('text',remove_html,'text')\npipeline+=pdp.ApplyByCols('text',remove_emoji,'text')\n#pipeline+=pdp.ApplyByCols('text',hashtag_to_words,'text')\npipeline+=pdp.ApplyByCols('text',remove_words_with_numbers,'text')\npipeline+=pdp.ApplyByCols('text',remove_numbers,'text')\npipeline+=pdp.ApplyByCols('text',remove_mentions,'text')\npipeline+=pdp.ApplyByCols('text',remove_square_bracket,'text')\npipeline+=pdp.ApplyByCols('text',remove_angular_bracket,'text')\npipeline+=pdp.ApplyByCols('text',Expand_Contractions,'text')\npipeline+=pdp.ApplyByCols('text',remove_punctuations,'text')\npipeline+=pdp.ApplyByCols('text',removeRepeated,'text')\npipeline+=pdp.ApplyByCols('text',convert_abbrev_in_text,'text')\npipeline+=pdp.ApplyByCols('text',correct_misspelled_with_context,'text')\npipeline+=pdp.ApplyByCols('text',remove_numbers,'text')\npipeline+=pdp.ApplyByCols('text',remove_stopwords,'text')\n#pipeline+=pdp.ApplyByCols('text',stemming_text,'text')\npipeline+=pdp.ApplyByCols('text',lemmatization,'text')\n\n\n\n\n# df=pd.concat([train,test],sort=False)\ntrain=pipeline(train)\ntest=pipeline(test)\ntest.head()","execution_count":57,"outputs":[{"output_type":"execute_result","execution_count":57,"data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                       happened terrible car crash \n1   2     NaN      NaN  heard earthquake different city stay safe ever...\n2   3     NaN      NaN  forest financial independence retire early spo...\n3   9     NaN      NaN              apocalypse lighting spokane wildfire \n4  11     NaN      NaN                   typhoon decor kill china taiwan ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>happened terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>heard earthquake different city stay safe ever...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>forest financial independence retire early spo...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>apocalypse lighting spokane wildfire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>typhoon decor kill china taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'","execution_count":98,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","execution_count":92,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT_COLUMN = 'text'\nTARGET_COLUMN = 'target'\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train[TEXT_COLUMN].astype(str)\ny_train = train[TARGET_COLUMN].values\nx_test = test[TEXT_COLUMN].astype(str)","execution_count":100,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE, lower=False)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","execution_count":101,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))","execution_count":104,"outputs":[{"output_type":"stream","text":"n unknown words (crawl):  173\nn unknown words (glove):  137\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape","execution_count":105,"outputs":[{"output_type":"execute_result","execution_count":105,"data":{"text/plain":"(11796, 600)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 300\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS","execution_count":106,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","execution_count":107,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embedding_matrix):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=result)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","execution_count":108,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_MODELS = 2\nBATCH_SIZE = 512\nEPOCHS = 5","execution_count":109,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean['location']=data_combined['location']\ndata_clean['keyword']=data_combined['keyword']\ndata_clean.head()","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"                                                text  \\\n0  our deeds are the reason of this earthquake ma...   \n1              forest fire near la ronge sask canada   \n2  all residents asked to shelter in place are be...   \n3   people receive wildfires evacuation orders in...   \n4  just got sent this photo from ruby alaska as s...   \n\n                                      text_tokenized location keyword  \n0  [deeds, reason, earthquake, may, allah, forgiv...      NaN     NaN  \n1      [forest, fire, near, la, ronge, sask, canada]      NaN     NaN  \n2  [residents, asked, shelter, place, notified, o...      NaN     NaN  \n3  [receive, wildfires, evacuation, orders, calif...      NaN     NaN  \n4  [got, sent, photo, ruby, alaska, smoke, wildfi...      NaN     NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_tokenized</th>\n      <th>location</th>\n      <th>keyword</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>our deeds are the reason of this earthquake ma...</td>\n      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>forest fire near la ronge sask canada</td>\n      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>all residents asked to shelter in place are be...</td>\n      <td>[residents, asked, shelter, place, notified, o...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>people receive wildfires evacuation orders in...</td>\n      <td>[receive, wildfires, evacuation, orders, calif...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>just got sent this photo from ruby alaska as s...</td>\n      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_predictions = []\nweights = []\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix)\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            x_train,y_train,\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=2\n        )\n        checkpoint_predictions.append(model.predict(x_test, batch_size=BATCH_SIZE))\n        weights.append(2 ** global_epoch)","execution_count":110,"outputs":[{"output_type":"stream","text":"Train on 7613 samples\n7613/7613 - 221s - loss: 0.5396\nTrain on 7613 samples\n7613/7613 - 213s - loss: 0.4415\nTrain on 7613 samples\n7613/7613 - 215s - loss: 0.4071\nTrain on 7613 samples\n7613/7613 - 226s - loss: 0.3901\nTrain on 7613 samples\n7613/7613 - 211s - loss: 0.3697\nTrain on 7613 samples\n7613/7613 - 212s - loss: 0.5522\nTrain on 7613 samples\n7613/7613 - 206s - loss: 0.4430\nTrain on 7613 samples\n7613/7613 - 207s - loss: 0.4128\nTrain on 7613 samples\n7613/7613 - 207s - loss: 0.3881\nTrain on 7613 samples\n7613/7613 - 205s - loss: 0.3724\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.average(checkpoint_predictions, weights=weights, axis=0)","execution_count":111,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.shape","execution_count":112,"outputs":[{"output_type":"execute_result","execution_count":112,"data":{"text/plain":"(3263, 1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsub.iloc[:, 1] = (predictions > 0.5).astype(int)","execution_count":115,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":116,"outputs":[{"output_type":"execute_result","execution_count":116,"data":{"text/plain":"   id  target\n0   0       1\n1   2       1\n2   3       1\n3   9       1\n4  11       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":117,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nCounter(sub['target'])","execution_count":118,"outputs":[{"output_type":"execute_result","execution_count":118,"data":{"text/plain":"Counter({1: 1199, 0: 2064})"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}