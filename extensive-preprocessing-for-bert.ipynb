{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for Word/Sentence Embedding Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting symspellpy\r\n",
      "  Downloading symspellpy-6.5.2-py3-none-any.whl (2.6 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 2.8 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /opt/conda/lib/python3.6/site-packages (from symspellpy) (1.18.2)\r\n",
      "Installing collected packages: symspellpy\r\n",
      "Successfully installed symspellpy-6.5.2\r\n",
      "Collecting pycontractions\r\n",
      "  Downloading pycontractions-2.0.1-py3-none-any.whl (9.6 kB)\r\n",
      "Requirement already satisfied: gensim>=2.0 in /opt/conda/lib/python3.6/site-packages (from pycontractions) (3.8.1)\r\n",
      "Requirement already satisfied: pyemd>=0.4.4 in /opt/conda/lib/python3.6/site-packages (from pycontractions) (0.5.1)\r\n",
      "Collecting language-check>=1.0\r\n",
      "  Downloading language-check-1.1.tar.gz (33 kB)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.4.1)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.10.0)\r\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.18.2)\r\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.26.0)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.12.32)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.22.0)\r\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.12.0)\r\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.5.0)\r\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.3.0)\r\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.32 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.15.32)\r\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.3.3)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.9.5)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2019.11.28)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.24.3)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (4.0.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.2.8)\r\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (46.1.3.post20200330)\r\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (4.0)\r\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.16.0)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.32->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8.1)\r\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.32->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.15.2)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.4.8)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.51.0)\r\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (3.11.3)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2019.3)\r\n",
      "Building wheels for collected packages: language-check\r\n",
      "  Building wheel for language-check (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for language-check: filename=language_check-1.1-py3-none-any.whl size=90190895 sha256=5ce9a1088c3980e8e7140ea1d1f0f267380608d58e6fe21f97168d5635da3cc5\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/fe/32/3b83a67c4f1182f7f6aa134c1d04cdcd893072bdadb4f5a64c\r\n",
      "Successfully built language-check\r\n",
      "Installing collected packages: language-check, pycontractions\r\n",
      "Successfully installed language-check-1.1 pycontractions-2.0.1\r\n",
      "Collecting keras-bert\r\n",
      "  Downloading keras-bert-0.81.0.tar.gz (29 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from keras-bert) (1.18.2)\r\n",
      "Requirement already satisfied: Keras in /opt/conda/lib/python3.6/site-packages (from keras-bert) (2.3.1)\r\n",
      "Collecting keras-transformer>=0.30.0\r\n",
      "  Downloading keras-transformer-0.33.0.tar.gz (11 kB)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from Keras->keras-bert) (5.3.1)\r\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from Keras->keras-bert) (1.0.8)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from Keras->keras-bert) (1.14.0)\r\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from Keras->keras-bert) (1.4.1)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from Keras->keras-bert) (1.1.0)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from Keras->keras-bert) (2.10.0)\r\n",
      "Collecting keras-pos-embd>=0.10.0\r\n",
      "  Downloading keras-pos-embd-0.11.0.tar.gz (5.9 kB)\r\n",
      "Collecting keras-multi-head>=0.22.0\r\n",
      "  Downloading keras-multi-head-0.22.0.tar.gz (12 kB)\r\n",
      "Collecting keras-layer-normalization>=0.12.0\r\n",
      "  Downloading keras-layer-normalization-0.14.0.tar.gz (4.3 kB)\r\n",
      "Collecting keras-position-wise-feed-forward>=0.5.0\r\n",
      "  Downloading keras-position-wise-feed-forward-0.6.0.tar.gz (4.4 kB)\r\n",
      "Collecting keras-embed-sim>=0.7.0\r\n",
      "  Downloading keras-embed-sim-0.7.0.tar.gz (4.1 kB)\r\n",
      "Collecting keras-self-attention==0.41.0\r\n",
      "  Downloading keras-self-attention-0.41.0.tar.gz (9.3 kB)\r\n",
      "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\r\n",
      "  Building wheel for keras-bert (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for keras-bert: filename=keras_bert-0.81.0-py3-none-any.whl size=37912 sha256=3e6c4890307160af9d4902e81fb6b799c6e6162c9161154e231368837a2eb177\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ee/d9/2a/75b40df359ab9096f06e55804ca64fbb2592a6ff77345c5fa7\r\n",
      "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.33.0-py3-none-any.whl size=13259 sha256=160a2d869cd1f439e72a4752526dde36ffcaee377c4be90ba7bd77d35833127f\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/f9/f8/2882032364dae9d3981b0ec988bb699980effe2a36a56e2248\r\n",
      "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-py3-none-any.whl size=7553 sha256=2dd35dc3eed55d19304037977b97ae0e2d7c96583be0de54890ced3babc575dc\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/13/b1/3b/13b632f78162148b123cddad1e0e3786df45ec37cac86dd998\r\n",
      "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-py3-none-any.whl size=15373 sha256=1943e2bda4b7087b8a0ea9b0335ac85c4299674f87c1b1872b82b42939b37294\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/43/20/55/ac730d8966dcc6fab002f2bf9be5eefecaae9eda8661586f2d\r\n",
      "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-py3-none-any.whl size=5267 sha256=900a2a062f2ca9bb3be6f25941af844b4bdb85ca227282d1bf3b6f8b6434942d\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/60/1a/38/858ffe627cf272dc54d9863d6c5ec993f00fd28d33f7f169f8\r\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-py3-none-any.whl size=5623 sha256=46f2ba69ef48cb23b2a0cd5940f939307f7054befaf85dee8fb284e53fe986ed\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/75/25/c7/5a4c25eabcddaa3f108a9fe5ad8f0ad94e6566f25c391ea4f6\r\n",
      "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-py3-none-any.whl size=4674 sha256=e6188fbebd9005ae3a0f8d8a90f5c215941e74d2363cfd1c53b737f0f7069eac\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/f3/16/a7/275994b075e49c199afced51712534a142429c90cd92a19241\r\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-py3-none-any.whl size=17288 sha256=303fea61162a986c90a00e05ca4968c04c0dd86ece0e769f06ace409717c219f\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/dc/71/4f/03604d0ee00490bf16606901a5977cd2bc3e4a087aff710e4f\r\n",
      "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\r\n",
      "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\r\n",
      "Successfully installed keras-bert-0.81.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.33.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install symspellpy\n",
    "!pip install pycontractions\n",
    "!pip install keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "import pkg_resources\n",
    "from symspellpy.symspellpy import SymSpell\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "#Contraction Import\n",
    "from pycontractions import Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spell checker for Segmentation and Spell Check tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "\n",
    "symspell_segmenter = SymSpell(max_dictionary_edit_distance=2, prefix_length=8)\n",
    "symspell_segmenter.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "sym_spell_misspelled = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "sym_spell_misspelled.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell_misspelled.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model Contractions Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====---------------------------------------------] 11.8% 45.7/387.1MB downloaded"
     ]
    }
   ],
   "source": [
    "cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0  1   NaN     NaN       \n",
       "1  4   NaN     NaN       \n",
       "2  5   NaN     NaN       \n",
       "3  6   NaN     NaN       \n",
       "4  7   NaN     NaN       \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                   \n",
       "1  Forest fire near La Ronge Sask. Canada                                                                                                  \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3  13,000 people receive #wildfires evacuation orders in California                                                                        \n",
       "4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                 \n",
       "\n",
       "   target  \n",
       "0  1       \n",
       "1  1       \n",
       "2  1       \n",
       "3  1       \n",
       "4  1       "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Let's load the data files\"\"\"\n",
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports to access Vocabulary file for BERT Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
      "1248387072/1248381879 [==============================] - 15s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras_bert import load_vocabulary, Tokenizer, get_checkpoint_paths\n",
    "from keras_bert.datasets import get_pretrained, PretrainedList\n",
    "model_path = get_pretrained(PretrainedList.wwm_uncased_large)\n",
    "paths = get_checkpoint_paths(model_path)\n",
    "token_dict = load_vocabulary(paths.vocab)\n",
    "tokenizer = Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+|pic.twitter.com\\S+')\n",
    "    return url.sub('[url]',text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_ucchar(text):\n",
    "    text = re.sub('&.*?;', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(r'@\\w*', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def handle_unicode(text):\n",
    "    text = text.encode('ascii', 'replace').decode('utf-8')\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    text = re.sub(r'([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_square_bracket(text):\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_angular_bracket(text):\n",
    "    text = re.sub('\\<.*?\\>+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_newline(text):\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_words_with_numbers(text):\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "def hashtag_to_words(text):\n",
    "    hashtag_list = re.findall(r\"#\\w+\",text)\n",
    "    for hashtag in hashtag_list:\n",
    "        hashtag = re.sub(r'#', '', hashtag)\n",
    "        text = re.sub(hashtag, symspell_segmenter.word_segmentation(hashtag).segmented_string, text)\n",
    "    text = re.sub(r'#', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def extra_spaces(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\s+|\\t+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        if token not in stopwords.words('english'):\n",
    "            textop = textop + token + ' '\n",
    "    return textop\n",
    "\n",
    "\n",
    "def correct_misspelled_with_context(text):\n",
    "    suggestions = sym_spell_misspelled.lookup_compound(text, max_edit_distance=2)\n",
    "    text = str(suggestions[0])\n",
    "    text = re.sub(r', \\d', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def stemming_text(text):\n",
    "    stemmer= PorterStemmer()\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        textop = textop + stemmer.stem(token) + ' '\n",
    "    return textop\n",
    "\n",
    "\n",
    "def lemmatization(text):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        textop = textop + lemmatizer.lemmatize(token) + ' '\n",
    "    return textop\n",
    "\n",
    "\n",
    "def removeRepeated(tweet):\n",
    "    prev = ''\n",
    "    tweet_new = ''\n",
    "    for c in tweet:\n",
    "        caps = False\n",
    "        if c.isdigit():\n",
    "            tweet_new += c\n",
    "            continue\n",
    "        if c.isalpha() == True:\n",
    "            if ord(c) >= 65 and ord(c)<=90:\n",
    "                caps = True\n",
    "            c = c.lower()\n",
    "            if c == prev:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "                prev = c\n",
    "            if count >= 3:\n",
    "                continue\n",
    "            if caps == True:\n",
    "                tweet_new += c.upper()\n",
    "            else:\n",
    "                tweet_new += c\n",
    "        else:\n",
    "            tweet_new += c\n",
    "    return tweet_new\n",
    "\n",
    "\n",
    "def Expand_Contractions(text):\n",
    "    return list(cont.expand_texts([text]))[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to extract vocabulary from a given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text, tokenizer=word_tokenize):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = text.apply(lambda x: tokenizer(x)).explode().value_counts().to_dict()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to check for coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to process the pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(text):\n",
    "    new_text = text.apply(lambda x : list(x)).explode()\n",
    "    return new_text.unique().shape[0]\n",
    "\n",
    "def count_words(text):\n",
    "    new_text = text.apply(lambda x : x.split(' ')).explode()\n",
    "    return new_text.unique().shape[0]\n",
    "\n",
    "def preprocess_pipeline(steps, col, df):\n",
    "    new_col = df[col]\n",
    "    char_count_before = 0\n",
    "    word_count_before = 0\n",
    "    char_count_after = 0\n",
    "    word_count_after = 0\n",
    "    for each_step in steps:\n",
    "        char_count_before = count_chars(new_col)\n",
    "        word_count_before = count_words(new_col)\n",
    "        new_col = new_col.apply(each_step)\n",
    "        char_count_after = count_chars(new_col)\n",
    "        word_count_after = count_words(new_col)\n",
    "        print(\"Preprocessing step: \",each_step.__name__)\n",
    "        print(\"Unique Char Count ---> Before: %d | After: %d\"%(char_count_before, char_count_after))\n",
    "        print(\"Unique Word Count ---> Before: %d | After: %d\"%(word_count_before, word_count_after))\n",
    "        vocab = build_vocab(new_col,word_tokenize)\n",
    "        check_coverage(vocab,token_dict)\n",
    "        print()\n",
    "    \n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define pipeline\n",
    "pipeline = []\n",
    "\n",
    "pipeline.append(handle_unicode)\n",
    "pipeline.append(to_lower)\n",
    "pipeline.append(remove_newline)\n",
    "pipeline.append(remove_url)\n",
    "pipeline.append(remove_special_ucchar)\n",
    "pipeline.append(hashtag_to_words)\n",
    "pipeline.append(remove_mentions)\n",
    "# pipeline.append(remove_square_bracket)\n",
    "# pipeline.append(remove_angular_bracket)\n",
    "pipeline.append(Expand_Contractions)\n",
    "# pipeline.append(remove_words_with_numbers)\n",
    "# pipeline.append(remove_punctuations)\n",
    "# pipeline.append(remove_punct)\n",
    "pipeline.append(extra_spaces)\n",
    "# pipeline.append(remove_numbers)\n",
    "# pipeline.append(removeRepeated)\n",
    "pipeline.append(correct_misspelled_with_context)\n",
    "# pipeline.append(remove_stopwords)\n",
    "# pipeline.append(stemming_text)\n",
    "# pipeline.append(lemmatization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the coverage of unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 23.46% of vocab\n",
      "Found embeddings for  66.31% of all text\n"
     ]
    }
   ],
   "source": [
    "# sentences = train[\"text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(train[\"text\"],word_tokenize)\n",
    "oov = check_coverage(vocab,token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the coverage of keywords from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = train.keyword.dropna().apply(lambda x: re.sub('%20',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 74.24% of vocab\n",
      "Found embeddings for  77.27% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(keywords,word_tokenize)\n",
    "oov = check_coverage(vocab,token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Training data:\n",
      "Preprocessing step:  handle_unicode\n",
      "Unique Char Count ---> Before: 122 | After: 94\n",
      "Unique Word Count ---> Before: 32017 | After: 32000\n",
      "Found embeddings for 23.85% of vocab\n",
      "Found embeddings for  67.23% of all text\n",
      "\n",
      "Preprocessing step:  to_lower\n",
      "Unique Char Count ---> Before: 94 | After: 68\n",
      "Unique Word Count ---> Before: 32000 | After: 28104\n",
      "Found embeddings for 37.41% of vocab\n",
      "Found embeddings for  84.56% of all text\n",
      "\n",
      "Preprocessing step:  remove_newline\n",
      "Unique Char Count ---> Before: 68 | After: 67\n",
      "Unique Word Count ---> Before: 28104 | After: 27967\n",
      "Found embeddings for 37.43% of vocab\n",
      "Found embeddings for  84.57% of all text\n",
      "\n",
      "Preprocessing step:  remove_url\n",
      "Unique Char Count ---> Before: 67 | After: 67\n",
      "Unique Word Count ---> Before: 27967 | After: 23383\n",
      "Found embeddings for 46.89% of vocab\n",
      "Found embeddings for  84.57% of all text\n",
      "\n",
      "Preprocessing step:  remove_special_ucchar\n",
      "Unique Char Count ---> Before: 67 | After: 66\n",
      "Unique Word Count ---> Before: 23383 | After: 23346\n",
      "Found embeddings for 46.89% of vocab\n",
      "Found embeddings for  84.43% of all text\n",
      "\n",
      "Preprocessing step:  hashtag_to_words\n",
      "Unique Char Count ---> Before: 66 | After: 65\n",
      "Unique Word Count ---> Before: 23346 | After: 22519\n",
      "Found embeddings for 48.04% of vocab\n",
      "Found embeddings for  84.51% of all text\n",
      "\n",
      "Preprocessing step:  remove_mentions\n",
      "Unique Char Count ---> Before: 65 | After: 64\n",
      "Unique Word Count ---> Before: 22519 | After: 20193\n",
      "Found embeddings for 54.69% of vocab\n",
      "Found embeddings for  85.72% of all text\n",
      "\n",
      "Preprocessing step:  Expand_Contractions\n",
      "Unique Char Count ---> Before: 64 | After: 65\n",
      "Unique Word Count ---> Before: 20193 | After: 20096\n",
      "Found embeddings for 54.76% of vocab\n",
      "Found embeddings for  86.56% of all text\n",
      "\n",
      "Preprocessing step:  extra_spaces\n",
      "Unique Char Count ---> Before: 65 | After: 65\n",
      "Unique Word Count ---> Before: 20096 | After: 20095\n",
      "Found embeddings for 54.76% of vocab\n",
      "Found embeddings for  86.56% of all text\n",
      "\n",
      "Preprocessing step:  correct_misspelled_with_context\n",
      "Unique Char Count ---> Before: 65 | After: 38\n",
      "Unique Word Count ---> Before: 20095 | After: 11588\n",
      "Found embeddings for 74.89% of vocab\n",
      "Found embeddings for  90.24% of all text\n",
      "\n",
      "CPU times: user 8min 49s, sys: 357 ms, total: 8min 49s\n",
      "Wall time: 9min 10s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake may allah forgive us all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la range sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>of of a people receive wildfires evacuation orders in california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as smoke from wildfires pours into a school</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0  1   NaN     NaN       \n",
       "1  4   NaN     NaN       \n",
       "2  5   NaN     NaN       \n",
       "3  6   NaN     NaN       \n",
       "4  7   NaN     NaN       \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                   \n",
       "1  Forest fire near La Ronge Sask. Canada                                                                                                  \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3  13,000 people receive #wildfires evacuation orders in California                                                                        \n",
       "4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                 \n",
       "\n",
       "   target  \\\n",
       "0  1        \n",
       "1  1        \n",
       "2  1        \n",
       "3  1        \n",
       "4  1        \n",
       "\n",
       "                                                                                                                         processed_text  \n",
       "0  our deeds are the reason of this earthquake may allah forgive us all                                                                  \n",
       "1  forest fire near la range sask canada                                                                                                 \n",
       "2  all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected    \n",
       "3  of of a people receive wildfires evacuation orders in california                                                                      \n",
       "4  just got sent this photo from ruby alaska as smoke from wildfires pours into a school                                                 "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "\n",
    "\n",
    "print(\"For Training data:\")\n",
    "train['processed_text'] = preprocess_pipeline(pipeline, 'text', train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                18                                                                          \n",
      "keyword           NaN                                                                         \n",
      "location          NaN                                                                         \n",
      "text              #raining #flooding #Florida #TampaBay #Tampa 18 or 19 days. I've lost count \n",
      "target            1                                                                           \n",
      "processed_text    raining flooding florida tampa bay tampa of or of days i have lost count    \n",
      "Name: 12, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train.loc[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Testing data:\n",
      "Preprocessing step:  handle_unicode\n",
      "Unique Char Count ---> Before: 118 | After: 92\n",
      "Unique Word Count ---> Before: 17426 | After: 17416\n",
      "Found embeddings for 28.58% of vocab\n",
      "Found embeddings for  66.75% of all text\n",
      "\n",
      "Preprocessing step:  to_lower\n",
      "Unique Char Count ---> Before: 92 | After: 66\n",
      "Unique Word Count ---> Before: 17416 | After: 15323\n",
      "Found embeddings for 46.26% of vocab\n",
      "Found embeddings for  84.40% of all text\n",
      "\n",
      "Preprocessing step:  remove_newline\n",
      "Unique Char Count ---> Before: 66 | After: 65\n",
      "Unique Word Count ---> Before: 15323 | After: 15306\n",
      "Found embeddings for 46.28% of vocab\n",
      "Found embeddings for  84.41% of all text\n",
      "\n",
      "Preprocessing step:  remove_url\n",
      "Unique Char Count ---> Before: 65 | After: 65\n",
      "Unique Word Count ---> Before: 15306 | After: 13280\n",
      "Found embeddings for 55.00% of vocab\n",
      "Found embeddings for  84.41% of all text\n",
      "\n",
      "Preprocessing step:  remove_special_ucchar\n",
      "Unique Char Count ---> Before: 65 | After: 64\n",
      "Unique Word Count ---> Before: 13280 | After: 13267\n",
      "Found embeddings for 54.99% of vocab\n",
      "Found embeddings for  84.27% of all text\n",
      "\n",
      "Preprocessing step:  hashtag_to_words\n",
      "Unique Char Count ---> Before: 64 | After: 63\n",
      "Unique Word Count ---> Before: 13267 | After: 12888\n",
      "Found embeddings for 56.43% of vocab\n",
      "Found embeddings for  84.40% of all text\n",
      "\n",
      "Preprocessing step:  remove_mentions\n",
      "Unique Char Count ---> Before: 63 | After: 62\n",
      "Unique Word Count ---> Before: 12888 | After: 11749\n",
      "Found embeddings for 62.75% of vocab\n",
      "Found embeddings for  85.72% of all text\n",
      "\n",
      "Preprocessing step:  Expand_Contractions\n",
      "Unique Char Count ---> Before: 62 | After: 63\n",
      "Unique Word Count ---> Before: 11749 | After: 11672\n",
      "Found embeddings for 62.84% of vocab\n",
      "Found embeddings for  86.52% of all text\n",
      "\n",
      "Preprocessing step:  extra_spaces\n",
      "Unique Char Count ---> Before: 63 | After: 63\n",
      "Unique Word Count ---> Before: 11672 | After: 11671\n",
      "Found embeddings for 62.84% of vocab\n",
      "Found embeddings for  86.52% of all text\n",
      "\n",
      "Preprocessing step:  correct_misspelled_with_context\n",
      "Unique Char Count ---> Before: 63 | After: 38\n",
      "Unique Word Count ---> Before: 11671 | After: 7632\n",
      "Found embeddings for 80.10% of vocab\n",
      "Found embeddings for  90.25% of all text\n",
      "\n",
      "CPU times: user 3min 51s, sys: 58.5 ms, total: 3min 51s\n",
      "Wall time: 3min 57s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "      <td>heard about earthquake is different cities stay safe everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "      <td>there is a forest fire at spot pond geese are fleeing across the street i cannot save them all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon so decor kills of in china and taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0  0   NaN     NaN       \n",
       "1  2   NaN     NaN       \n",
       "2  3   NaN     NaN       \n",
       "3  9   NaN     NaN       \n",
       "4  11  NaN     NaN       \n",
       "\n",
       "                                                                                               text  \\\n",
       "0  Just happened a terrible car crash                                                                 \n",
       "1  Heard about #earthquake is different cities, stay safe everyone.                                   \n",
       "2  there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all   \n",
       "3  Apocalypse lighting. #Spokane #wildfires                                                           \n",
       "4  Typhoon Soudelor kills 28 in China and Taiwan                                                      \n",
       "\n",
       "                                                                                     processed_text  \n",
       "0  just happened a terrible car crash                                                                \n",
       "1  heard about earthquake is different cities stay safe everyone                                     \n",
       "2  there is a forest fire at spot pond geese are fleeing across the street i cannot save them all    \n",
       "3  apocalypse lighting spokane wildfires                                                             \n",
       "4  typhoon so decor kills of in china and taiwan                                                     "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "\n",
    "print(\"For Testing data:\")\n",
    "test['processed_text'] = preprocess_pipeline(pipeline, 'text', test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['o', 'u', 'r', ' ', 'd', 'e', 's', 'a', 't', 'h', 'n', 'f', 'i',\n",
       "       'q', 'k', 'm', 'y', 'l', 'g', 'v', 'c', 'p', 'b', 'x', 'w', 'j',\n",
       "       '0', '2', '4', '3', '6', '7', '1', '8', '9', '5', 'z', \"'\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = train['processed_text'].apply(lambda x : list(x)).explode()\n",
    "chars.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention all RCHS football players there will be coffins and body bags by the locker rooms grab one tommorow because were gonna die\n",
      "attention all chs football players there will be coffins and body bags by the locker rooms grab one tomorrow because were going to die  \n"
     ]
    }
   ],
   "source": [
    "print(train['text'].iloc[1031])\n",
    "print(train['processed_text'].iloc[1031])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the kernel - https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data\n",
    "\n",
    "The author of the above kernel, manually read the tweets from training data and figured out that some of them were misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "train.loc[train['id'].isin(ids_with_target_error),'target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, idx = np.unique(train['processed_text'], return_index=True)\n",
    "train = train.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "tweet_len = train['processed_text'].apply(len)\n",
    "print(tweet_len.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "tweet_len = test['processed_text'].apply(len)\n",
    "print(tweet_len.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('processed train.csv', index=False)\n",
    "test.to_csv('processed test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
