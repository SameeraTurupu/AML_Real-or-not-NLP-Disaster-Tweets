{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\r\n",
      "  Downloading bert-for-tf2-0.14.1.tar.gz (40 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 40 kB 978 kB/s \r\n",
      "\u001b[?25hCollecting py-params>=0.9.6\r\n",
      "  Downloading py-params-0.9.6.tar.gz (6.6 kB)\r\n",
      "Collecting params-flow>=0.8.0\r\n",
      "  Downloading params-flow-0.8.0.tar.gz (19 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.42.0)\r\n",
      "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\r\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.1-py3-none-any.whl size=30084 sha256=db5e6281f20c901a081f9415d98ffb509e285ed90d1573c99d6edab6bd6359f6\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/ae/77/ee98e1d17085ed67e01e617b7e5a24f320d3f00ad79846c939\r\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for py-params: filename=py_params-0.9.6-py3-none-any.whl size=7088 sha256=c55690283318b2d5fb18230ed198178592183d187b6e8472042c91305fdb7ab5\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/4d/5a/7fbbdfb87bf3da12265e0d79dc19ce143652a09e3696189eb8\r\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.8.0-py3-none-any.whl size=15999 sha256=4b5be7026f0052136e056dc57deaa915f9b03ef602384a93b70a90f1713c0a85\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/f7/ff/d3/cb47aae1024dbabf32cd5b983aa68042fa7bbcd173fcdc162c\r\n",
      "Successfully built bert-for-tf2 py-params params-flow\r\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\r\n",
      "Successfully installed bert-for-tf2-0.14.1 params-flow-0.8.0 py-params-0.9.6\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.85)\r\n",
      "Collecting pdpipe\r\n",
      "  Downloading pdpipe-0.0.46-py3-none-any.whl (47 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 47 kB 629 kB/s \r\n",
      "\u001b[?25hCollecting strct\r\n",
      "  Downloading strct-0.0.30-py2.py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pdpipe) (4.42.0)\r\n",
      "Collecting skutil>=0.0.15\r\n",
      "  Downloading skutil-0.0.16-py2.py3-none-any.whl (18 kB)\r\n",
      "Requirement already satisfied: sortedcontainers in /opt/conda/lib/python3.6/site-packages (from pdpipe) (2.1.0)\r\n",
      "Requirement already satisfied: pandas>=0.18.0 in /opt/conda/lib/python3.6/site-packages (from pdpipe) (0.25.3)\r\n",
      "Collecting decore\r\n",
      "  Downloading decore-0.0.1.tar.gz (19 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from skutil>=0.0.15->pdpipe) (1.18.2)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.18.0->pdpipe) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.18.0->pdpipe) (2.8.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.18.0->pdpipe) (1.14.0)\r\n",
      "Building wheels for collected packages: decore\r\n",
      "  Building wheel for decore (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for decore: filename=decore-0.0.1-py2.py3-none-any.whl size=4190 sha256=7b341625d62bf82afb1292a60c454c5293433aed3f90c316f31a2dfdb9c2c854\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/40/2a/59306076795b4b9b35a210a859053a479061021810c546ddbe\r\n",
      "Successfully built decore\r\n",
      "Installing collected packages: strct, decore, skutil, pdpipe\r\n",
      "Successfully installed decore-0.0.1 pdpipe-0.0.46 skutil-0.0.16 strct-0.0.30\r\n",
      "Collecting symspellpy\r\n",
      "  Downloading symspellpy-6.5.2-py3-none-any.whl (2.6 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 1.4 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /opt/conda/lib/python3.6/site-packages (from symspellpy) (1.18.2)\r\n",
      "Installing collected packages: symspellpy\r\n",
      "Successfully installed symspellpy-6.5.2\r\n",
      "Collecting pycontractions\r\n",
      "  Downloading pycontractions-2.0.1-py3-none-any.whl (9.6 kB)\r\n",
      "Collecting language-check>=1.0\r\n",
      "  Downloading language-check-1.1.tar.gz (33 kB)\r\n",
      "Requirement already satisfied: gensim>=2.0 in /opt/conda/lib/python3.6/site-packages (from pycontractions) (3.8.1)\r\n",
      "Requirement already satisfied: pyemd>=0.4.4 in /opt/conda/lib/python3.6/site-packages (from pycontractions) (0.5.1)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.10.0)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.4.1)\r\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.18.2)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.12.32)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.22.0)\r\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.26.0)\r\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.32 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.15.32)\r\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.3.3)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.9.5)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2019.11.28)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.24.3)\r\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.3.0)\r\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.12.0)\r\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.5.0)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.32->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8.1)\r\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.32->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.15.2)\r\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.16.0)\r\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (4.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.2.8)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (4.0.0)\r\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (46.1.3.post20200330)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2019.3)\r\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (3.11.3)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.51.0)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.4.8)\r\n",
      "Building wheels for collected packages: language-check\r\n",
      "  Building wheel for language-check (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for language-check: filename=language_check-1.1-py3-none-any.whl size=90190895 sha256=14f59ce2459c2574b725deef06698269038783418bdbbf4fb560c6231965ea29\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/fe/32/3b83a67c4f1182f7f6aa134c1d04cdcd893072bdadb4f5a64c\r\n",
      "Successfully built language-check\r\n",
      "Installing collected packages: language-check, pycontractions\r\n",
      "Successfully installed language-check-1.1 pycontractions-2.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece\n",
    "!pip install pdpipe \n",
    "!pip install symspellpy\n",
    "!pip install pycontractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "\n",
    "\n",
    "#data pipeline \n",
    "import pdpipe as pdp\n",
    "\n",
    "#Other Preprocessing\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import string\n",
    "# !pip install symspellpy\n",
    "import pkg_resources\n",
    "from symspellpy.symspellpy import SymSpell\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Contraction Import\n",
    "from pycontractions import Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, json, csv\n",
    "import re\n",
    "\n",
    "def remove_wrong_abb(key,value):\n",
    "    if(re.search('it means',value)):\n",
    "        return False;\n",
    "    elif(re.search('\\*',value)):\n",
    "        return False\n",
    "    elif(value==\"\"):\n",
    "        return False\n",
    "    elif(len(key)>7):\n",
    "        return False\n",
    "    else :\n",
    "        return True\n",
    "    \n",
    "resp = requests.get(\"http://www.netlingo.com/acronyms.php\")\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "slangdict= {}\n",
    "key=\"\"\n",
    "value=\"\"\n",
    "for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "    for li in div.findAll('li'):\n",
    "        for a in li.findAll('a'):\n",
    "            key =a.text\n",
    "            value = li.text.split(key)[1]\n",
    "            if(remove_wrong_abb(key,value)):\n",
    "                if(re.search('\\ -or- ',value)):\n",
    "                    pos=re.search('\\ -or- ',value).start()\n",
    "                    slangdict[key]=value[:pos]\n",
    "                else:\n",
    "                    slangdict[key]=value\n",
    "\n",
    "                    \n",
    "# with open('myslang.json', 'w') as f:\n",
    "#     json.dump(slangdict, f, indent=2)\n",
    "\n",
    "w = csv.writer(open(\"myslang.csv\", \"w\"))\n",
    "for key, val in slangdict.items():\n",
    "    w.writerow([key.lower(), val.lower()])\n",
    "    \n",
    "reader = csv.reader(open('myslang.csv', 'r'))\n",
    "abbreviations = {}\n",
    "for row in reader:\n",
    "    k, v = row\n",
    "    abbreviations[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym_spell_4space = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell_4space.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "\n",
    "sym_spell_misspelled = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell_misspelled.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell_misspelled.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====---------------------------------------------] 11.8% 45.8/387.1MB downloaded"
     ]
    }
   ],
   "source": [
    "cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version Edit: Sonam D.\n",
    "def to_lower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+|pic.twitter.com\\S+')\n",
    "    return url.sub(r' ',text)\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r' ',text)\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', text)\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_numbers(text):\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(r'@\\w*', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_punctuations(text):\n",
    "    text = re.sub(r'([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_square_bracket(text):\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_angular_bracket(text):\n",
    "    text = re.sub('\\<.*?\\>+', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_newline(text):\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_words_with_numbers(text):\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    return text\n",
    "    \n",
    "#Version Edit: Susanth D.\n",
    "def hashtag_to_words(text):\n",
    "    text = re.sub(r'##', '#', text)\n",
    "    hash_pattern = re.compile(r\"#\\w*\")\n",
    "    hashtag_list = re.findall(r\"#\\w+\",text)\n",
    "    for hashtag in hashtag_list:\n",
    "        hashtag = re.sub(r'#', '', hashtag)\n",
    "        text = re.sub(hashtag, sym_spell_4space.word_segmentation(hashtag).corrected_string, text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "#     text = re.sub(r'# ', '', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_stopwords(text):\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        if token not in stopwords.words('english'):\n",
    "            textop = textop + token + ' '\n",
    "    return textop\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def correct_misspelled_with_context(text):\n",
    "    suggestions = sym_spell_misspelled.lookup_compound(text, max_edit_distance=2)\n",
    "    text = str(suggestions[0])\n",
    "    text = re.sub(r', \\d', ' ', text)\n",
    "#     text = remove_numbers(text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def stemming_text(text):\n",
    "    stemmer= PorterStemmer()\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        textop = textop + stemmer.stem(token) + ' '\n",
    "    return textop\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def lemmatization(text):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        textop = textop + lemmatizer.lemmatize(token) + ' '\n",
    "    return textop\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def convert_abbrev(word):\n",
    "    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def convert_abbrev_in_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [convert_abbrev(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "# def handle_contractions(text):\n",
    "#     text = re.sub(r\"’\", \"'\", text)\n",
    "#     for word in text.split():\n",
    "#         if word.lower() in contractions:\n",
    "#             text = text.replace(word, contractions[word.lower()])\n",
    "#     return(text)\n",
    "\n",
    "#Version Edit: Saurabh M.\n",
    "def removeRepeated(tweet):\n",
    "    prev = ''\n",
    "    tweet_new = ''\n",
    "    for c in tweet:\n",
    "        caps = False\n",
    "        if c.isdigit():\n",
    "            tweet_new += c\n",
    "            continue\n",
    "        if c.isalpha() == True:\n",
    "            if ord(c) >= 65 and ord(c)<=90:\n",
    "                caps = True\n",
    "            c = c.lower()\n",
    "            if c == prev:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "                prev = c\n",
    "            if count >= 3:\n",
    "                continue\n",
    "            if caps == True:\n",
    "                tweet_new += c.upper()\n",
    "            else:\n",
    "                tweet_new += c\n",
    "        else:\n",
    "            tweet_new += c\n",
    "    return tweet_new\n",
    "\n",
    "\n",
    "def Expand_Contractions(text):\n",
    "    return list(cont.expand_texts([text]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(text):\n",
    "    new_text = text.apply(lambda x : list(x)).explode()\n",
    "    return new_text.unique().shape[0]\n",
    "\n",
    "def count_words(text):\n",
    "    new_text = text.apply(lambda x : x.split(' ')).explode()\n",
    "    return new_text.unique().shape[0]\n",
    "\n",
    "def preprocess_pipeline(steps, col, df):\n",
    "    new_col = df[col]\n",
    "    char_count_before = 0\n",
    "    word_count_before = 0\n",
    "    char_count_after = 0\n",
    "    word_count_after = 0\n",
    "    for each_step in steps:\n",
    "        char_count_before = count_chars(new_col)\n",
    "        word_count_before = count_words(new_col)\n",
    "        new_col = new_col.apply(each_step)\n",
    "        char_count_after = count_chars(new_col)\n",
    "        word_count_after = count_words(new_col)\n",
    "        print(\"Preprocessing step: \",each_step.__name__)\n",
    "        print(\"Char Count ---> Before: %d | After: %d\"%(char_count_before, char_count_after))\n",
    "        print(\"Word Count ---> Before: %d | After: %d\"%(word_count_before, word_count_after))\n",
    "    \n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing step:  remove_newline\n",
      "Char Count ---> Before: 122 | After: 121\n",
      "Word Count ---> Before: 32017 | After: 31925\n",
      "Preprocessing step:  remove_URL\n",
      "Char Count ---> Before: 121 | After: 121\n",
      "Word Count ---> Before: 31925 | After: 27333\n",
      "Preprocessing step:  remove_html\n",
      "Char Count ---> Before: 121 | After: 121\n",
      "Word Count ---> Before: 27333 | After: 27333\n",
      "Preprocessing step:  remove_emoji\n",
      "Char Count ---> Before: 121 | After: 121\n",
      "Word Count ---> Before: 27333 | After: 27333\n",
      "Preprocessing step:  remove_mentions\n",
      "Char Count ---> Before: 121 | After: 120\n",
      "Word Count ---> Before: 27333 | After: 25007\n",
      "Preprocessing step:  remove_square_bracket\n",
      "Char Count ---> Before: 120 | After: 120\n",
      "Word Count ---> Before: 25007 | After: 24872\n",
      "Preprocessing step:  remove_angular_bracket\n",
      "Char Count ---> Before: 120 | After: 120\n",
      "Word Count ---> Before: 24872 | After: 24872\n",
      "Preprocessing step:  Expand_Contractions\n",
      "Char Count ---> Before: 120 | After: 120\n",
      "Word Count ---> Before: 24872 | After: 24700\n",
      "Preprocessing step:  removeRepeated\n",
      "Char Count ---> Before: 120 | After: 119\n",
      "Word Count ---> Before: 24700 | After: 24711\n",
      "Preprocessing step:  lemmatization\n",
      "Char Count ---> Before: 119 | After: 119\n",
      "Word Count ---> Before: 24711 | After: 19428\n",
      "Preprocessing step:  remove_newline\n",
      "Char Count ---> Before: 118 | After: 117\n",
      "Word Count ---> Before: 17426 | After: 17448\n",
      "Preprocessing step:  remove_URL\n",
      "Char Count ---> Before: 117 | After: 117\n",
      "Word Count ---> Before: 17448 | After: 15417\n",
      "Preprocessing step:  remove_html\n",
      "Char Count ---> Before: 117 | After: 117\n",
      "Word Count ---> Before: 15417 | After: 15417\n",
      "Preprocessing step:  remove_emoji\n",
      "Char Count ---> Before: 117 | After: 117\n",
      "Word Count ---> Before: 15417 | After: 15417\n",
      "Preprocessing step:  remove_mentions\n",
      "Char Count ---> Before: 117 | After: 116\n",
      "Word Count ---> Before: 15417 | After: 14278\n",
      "Preprocessing step:  remove_square_bracket\n",
      "Char Count ---> Before: 116 | After: 115\n",
      "Word Count ---> Before: 14278 | After: 14197\n",
      "Preprocessing step:  remove_angular_bracket\n",
      "Char Count ---> Before: 115 | After: 115\n",
      "Word Count ---> Before: 14197 | After: 14197\n",
      "Preprocessing step:  Expand_Contractions\n",
      "Char Count ---> Before: 115 | After: 115\n",
      "Word Count ---> Before: 14197 | After: 14071\n",
      "Preprocessing step:  removeRepeated\n",
      "Char Count ---> Before: 115 | After: 113\n",
      "Word Count ---> Before: 14071 | After: 14085\n",
      "Preprocessing step:  lemmatization\n",
      "Char Count ---> Before: 113 | After: 113\n",
      "Word Count ---> Before: 14085 | After: 11593\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "\n",
    "\n",
    "pipeline = []\n",
    "# pipeline.append(to_lower)\n",
    "pipeline.append(remove_newline)\n",
    "pipeline.append(remove_URL)\n",
    "pipeline.append(remove_html)\n",
    "pipeline.append(remove_emoji)\n",
    "# pipeline.append(hashtag_to_words)\n",
    "# pipeline.append(remove_words_with_numbers)\n",
    "# pipeline.append(remove_numbers)\n",
    "pipeline.append(remove_mentions)\n",
    "pipeline.append(remove_square_bracket)\n",
    "pipeline.append(remove_angular_bracket)\n",
    "pipeline.append(Expand_Contractions)\n",
    "# pipeline.append(remove_punctuations)\n",
    "pipeline.append(removeRepeated)\n",
    "# pipeline.append(convert_abbrev_in_text)\n",
    "# pipeline.append(remove_stopwords)\n",
    "# pipeline.append(correct_misspelled_with_context)\n",
    "# pipeline.append(remove_numbers)\n",
    "# pipeline.append(remove_stopwords)\n",
    "# # pipeline.append(stemming_text)\n",
    "pipeline.append(lemmatization)\n",
    "\n",
    "train['processed_text'] = preprocess_pipeline(pipeline, 'text', train)\n",
    "test['processed_text'] = preprocess_pipeline(pipeline, 'text', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.copy()\n",
    "df_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Our Deeds are the Reason of this # earthquake ...\n",
       "1                Forest fire near La Ronge Sask . Canada \n",
       "2       All resident asked to 'shelter in place ' are ...\n",
       "3       13,000 people receive # wildfire evacuation or...\n",
       "4       Just got sent this photo from Ruby # Alaska a ...\n",
       "                              ...                        \n",
       "7608    Two giant crane holding a bridge collapse into...\n",
       "7609    The out of control wild fire in California eve...\n",
       "7610                   M1.94 ? 5km S of Volcano Hawaii . \n",
       "7611    Police investigating after an e-bike collided ...\n",
       "7612    The Latest : More Homes Razed by Northern Cali...\n",
       "Name: processed_text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "\n",
    "X_all = pd.concat([df[\"processed_text\"], df_test[\"processed_text\"]])\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "tfidf.fit(X_all)\n",
    "\n",
    "X = tfidf.transform(df[\"processed_text\"])\n",
    "X_test = tfidf.transform(df_test[\"processed_text\"])\n",
    "del X_all\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "train_x = train[\"processed_text\"]\n",
    "train_y = train[\"target\"]\n",
    "\n",
    "test_x = test[\"processed_text\"]\n",
    "test_y = test[\"target\"]\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { \n",
    "    'gamma': [0.001, 0.01, 0.1, 0.4, 0.5, 0.6, 0.7, 1], \n",
    "    'kernel': ['rbf'], \n",
    "    'C': [0.001, 0.01, 0.1, 1, 1.5, 2, 3, 10],\n",
    "}\n",
    "model = GridSearchCV(SVC(), parameters, cv=10, n_jobs=-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.5, 'gamma': 0.5, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cv_results_['params'][model.best_index_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7874015748031497, 0.7263513513513513)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_pred = model.predict(X_val)\n",
    "accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[385,  41],\n",
       "       [121, 215]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(os.path.join('../input/nlp-getting-started/', 'sample_submission.csv'))\n",
    "sub_df[\"target\"] = y_test_pred\n",
    "sub_df.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
