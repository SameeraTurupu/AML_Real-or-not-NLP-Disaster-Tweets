{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\r\n",
      "  Downloading bert-for-tf2-0.14.4.tar.gz (40 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 40 kB 1.8 MB/s \r\n",
      "\u001b[?25hCollecting py-params>=0.9.6\r\n",
      "  Downloading py-params-0.9.7.tar.gz (6.8 kB)\r\n",
      "Collecting params-flow>=0.8.0\r\n",
      "  Downloading params-flow-0.8.1.tar.gz (19 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.42.0)\r\n",
      "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\r\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-py3-none-any.whl size=30114 sha256=9d4901b76f5faca5fada693ff88ee92b232021d88aae664923ef1f6f2c97ed18\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/b5/ea/3e768502fa1032a6bfefcd0e7dd95e2110e026bf3c584940cd\r\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for py-params: filename=py_params-0.9.7-py3-none-any.whl size=7302 sha256=30a8b0985f4d927afbd817f59233db1b041f71331f1eb83b0bf37010f6afb287\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/e2/80/244168a554fc8469897bb86b3ea7183b56ecc92f484e67b375\r\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.8.1-py3-none-any.whl size=16121 sha256=e629d1a58f37bca7b1b4d2ab6cfb5262a57cb5cedde02cdbf5042b5b728c3449\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/cc/70/6f/98e4cd9239940e8160866872af2be0ca038cdfbdc8113db2ad\r\n",
      "Successfully built bert-for-tf2 py-params params-flow\r\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\r\n",
      "Successfully installed bert-for-tf2-0.14.4 params-flow-0.8.1 py-params-0.9.7\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.85)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/extensive-pre-processing-for-bert/processed train.csv\n",
      "/kaggle/input/extensive-pre-processing-for-bert/processed test.csv\n",
      "/kaggle/input/extensive-pre-processing-for-bert/custom.css\n",
      "/kaggle/input/extensive-pre-processing-for-bert/__notebook__.ipynb\n",
      "/kaggle/input/extensive-pre-processing-for-bert/__results__.html\n",
      "/kaggle/input/extensive-pre-processing-for-bert/__output__.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from tqdm import tqdm\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "# from keras.initializers import Constant\n",
    "# from keras.optimizers import Adam\n",
    "# from keras import regularizers\n",
    "# import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "#     y_true = K.argmax(y_true_in, axis=1)\n",
    "#     y_pred = K.argmax(y_pred_in, axis=1)\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10517</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>! Residents Return To Destroyed Homes As Washi...</td>\n",
       "      <td>1</td>\n",
       "      <td>! residents return to destroyed homes as washi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7768</td>\n",
       "      <td>police</td>\n",
       "      <td>USA</td>\n",
       "      <td>$1 million bail for man accused of #shooting a...</td>\n",
       "      <td>1</td>\n",
       "      <td>$1 million bail for man accused of shooting at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3990</td>\n",
       "      <td>devastation</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>$10M SETTLEMENT attained using our illustratio...</td>\n",
       "      <td>1</td>\n",
       "      <td>$10m settlement attained using our illustratio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>178</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>United States</td>\n",
       "      <td>&amp;gt;&amp;gt; $15 Aftershock : Protect Yourself and...</td>\n",
       "      <td>0</td>\n",
       "      <td>$15 aftershock : protect yourself and profit i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2657</td>\n",
       "      <td>crush</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'@jorrynja: 6. @ your bf/gf/crush ??' @Ter_ell ??</td>\n",
       "      <td>1</td>\n",
       "      <td>' : 6. your bf/gf/crush ??' ??</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      keyword       location  \\\n",
       "0  10517     wildfire            NaN   \n",
       "1   7768       police            USA   \n",
       "2   3990  devastation     Denver, CO   \n",
       "3    178   aftershock  United States   \n",
       "4   2657        crush            NaN   \n",
       "\n",
       "                                                text  target  \\\n",
       "0  ! Residents Return To Destroyed Homes As Washi...       1   \n",
       "1  $1 million bail for man accused of #shooting a...       1   \n",
       "2  $10M SETTLEMENT attained using our illustratio...       1   \n",
       "3  &gt;&gt; $15 Aftershock : Protect Yourself and...       0   \n",
       "4  '@jorrynja: 6. @ your bf/gf/crush ??' @Ter_ell ??       1   \n",
       "\n",
       "                                      processed_text  \n",
       "0  ! residents return to destroyed homes as washi...  \n",
       "1  $1 million bail for man accused of shooting at...  \n",
       "2  $10m settlement attained using our illustratio...  \n",
       "3  $15 aftershock : protect yourself and profit i...  \n",
       "4                     ' : 6. your bf/gf/crush ??' ??  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= pd.read_csv('../input/extensive-pre-processing-for-bert/processed train.csv')\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"' : 6. your bf/gf/crush ??' ??\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[4,'processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard about earthquake is different cities, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting. spokane wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kills 28 in china and taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "id                                                                           \n",
       "0    0     NaN      NaN                 Just happened a terrible car crash   \n",
       "2    2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "3    3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "9    9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "11  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                       processed_text  \n",
       "id                                                     \n",
       "0                  just happened a terrible car crash  \n",
       "2   heard about earthquake is different cities, st...  \n",
       "3   there is a forest fire at spot pond, geese are...  \n",
       "9              apocalypse lighting. spokane wildfires  \n",
       "11      typhoon soudelor kills 28 in china and taiwan  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.read_csv('../input/extensive-pre-processing-for-bert/processed test.csv')\n",
    "test = test.set_index(test['id'])\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "# module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "#https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\n",
    "#https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/1\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BERT Methods Predefined\"\"\"\n",
    "def bert_encode(texts, tokenizer, max_len=50):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6934 rows and 6 columns in train\n",
      "There are 3263 rows and 5 columns in train\n"
     ]
    }
   ],
   "source": [
    "print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\n",
    "print('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_input = bert_encode(train.processed_text.values, tokenizer, max_len=50)\n",
    "full_labels = train.target.values.copy()\n",
    "full_labels_oe = to_categorical(full_labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a374ec2c1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_labels_oe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mval_labels_oe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_labels' is not defined"
     ]
    }
   ],
   "source": [
    "train_data, val_data, train_labels, val_labels = train_test_split(train.processed_text.values, train.target.values, test_size=0.15, random_state=10)\n",
    "train_input = bert_encode(train_data, tokenizer, max_len=50)\n",
    "val_input = bert_encode(val_data, tokenizer, max_len=50)\n",
    "test_input = bert_encode(test.processed_text.values, tokenizer, max_len=50)\n",
    "\n",
    "test_labels_oe = to_categorical(test_labels, 2)\n",
    "val_labels_oe = to_categorical(val_labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=9e-6\n",
    "decay=9e-2\n",
    "max_len=50\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "clf_output = sequence_output[:, 0, :]\n",
    "# clf_pooled = pooled_output[:, 0, :]\n",
    "\n",
    "## Type1\n",
    "# out = Dense(100, activation='relu', activity_regularizer=regularizers.l2(9e-5))(clf_output)\n",
    "# out = Dense(100, activation='relu', activity_regularizer=regularizers.l2(9e-5))(out)\n",
    "# out = Dense(100, activation='relu')(out)\n",
    "\n",
    "## Type2\n",
    "# out = Dropout(Dropout_num)(clf_output)\n",
    "out = Dense(1024, activation='relu', activity_regularizer=regularizers.l2(9e-5))(clf_output)\n",
    "out = Dense(1024, activation='relu', activity_regularizer=regularizers.l2(9e-5))(out)\n",
    "out = Dense(1024, activation='relu')(out)\n",
    "\n",
    "## Type3\n",
    "# out = clf_output\n",
    "\n",
    "\n",
    "out = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "\n",
    "## Type4\n",
    "# out = clf_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         1049600     tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         1049600     dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         1049600     dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            1025        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 338,291,714\n",
      "Trainable params: 338,291,713\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sBERT = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "# sBERT.compile(SGD(lr=learning_rate, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "sBERT.compile(Adam(lr=learning_rate, decay=decay), loss='binary_crossentropy', metrics=['accuracy',f1_m])\n",
    "sBERT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_weights = sBERT.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sBERT.set_weights(init_weights)\n",
    "# sBERT.layers[3].trainable = True\n",
    "# sBERT.compile(Adam(lr=9e-6, decay=9e-2), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6934 samples\n",
      "Epoch 1/3\n",
      "6934/6934 [==============================] - 6690s 965ms/sample - loss: 0.5882 - accuracy: 0.7391 - f1_m: 0.5409\n",
      "Epoch 2/3\n",
      "6934/6934 [==============================] - 6709s 967ms/sample - loss: 0.5250 - accuracy: 0.7850 - f1_m: 0.6779\n",
      "Epoch 3/3\n",
      "6934/6934 [==============================] - 6713s 968ms/sample - loss: 0.5069 - accuracy: 0.7900 - f1_m: 0.6897\n"
     ]
    }
   ],
   "source": [
    "checkpoint1 = ModelCheckpoint('best_accuracy.h5',\n",
    "                             monitor='val_f1_m',\n",
    "                             save_best_only=True)\n",
    "checkpoint2 = ModelCheckpoint('best_loss.h5',\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True)\n",
    "\n",
    "\n",
    "train_history = sBERT.fit(\n",
    "    full_input, full_labels,\n",
    "    epochs = 3,\n",
    "    batch_size = 12\n",
    "#     callbacks = [checkpoint1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # K.set_value(sBERT.optimizer.lr, 9e-7)\n",
    "# train_history = sBERT.fit(\n",
    "#     full_input, full_labels,\n",
    "#     epochs = 3,\n",
    "#     batch_size = 16\n",
    "# #     callbacks = [checkpoint1]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sBERT.layers[3].trainable = False\n",
    "# sBERT.compile(Adam(lr=5e-4, decay=5e-2), loss='binary_crossentropy', metrics=['accuracy',f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # K.set_value(sBERT.optimizer.lr, 5e-3)\n",
    "# sBERT.fit(\n",
    "#     full_input, full_labels,\n",
    "#     epochs = 5,\n",
    "#     callbacks=[checkpoint1],\n",
    "#     batch_size = 16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sBERT.load_weights('testacc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "==================================================================================================\n",
      "Total params: 335,141,889\n",
      "Trainable params: 335,141,888\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_encoder = Model(sBERT.inputs, sBERT.layers[4].output)\n",
    "bert_encoder.summary()\n",
    "\n",
    "# bert_encoder.layers[-3].set_weights(sBERT.layers[-4].get_weights())\n",
    "# bert_encoder.layers[-2].set_weights(sBERT.layers[-3].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 37min 29s, sys: 59.3 s, total: 4h 38min 28s\n",
      "Wall time: 1h 12min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_embed = bert_encoder.predict(full_input)\n",
    "train_embed = bert_encoder.predict(train_input)\n",
    "test_embed = bert_encoder.predict(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle BERT Encodings for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Train BERT 1024d Embed', 'ab') as fo:     \n",
    "    pickle.dump(full_embed, fo)    \n",
    "\n",
    "with open('Test BERT 1024d Embed', 'ab') as fo:     \n",
    "    pickle.dump(test_embed, fo)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Train BERT 1024d Embed', 'rb') as fo:\n",
    "#     tt = pickle.load(fo, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 65.6 ms, total: 1min 20s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=3, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svc_model = SVC(gamma=0.01, kernel='rbf', C=3)\n",
    "svc_model.fit(full_embed, full_labels)\n",
    "# svc_model.fit(train_embed, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5893, 6934]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                        zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1224\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1484\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                          str(average_options))\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5893, 6934]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth=200, n_estimators=400, subsample=1, learning_rate=0.07, reg_lambda=0.1, reg_alpha=0.1,\\\n",
    "                       gamma=1)\n",
    "clf.fit(full_embed, full_labels)\n",
    "# clf.fit(train_embed, train_labels)\n",
    "\n",
    "predictions = clf.predict(full_embed)\n",
    "# predictions = clf.predict(train_embed)\n",
    "print (\"Training set f1_score :\", np.round(f1_score(train_labels, predictions),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-197614da95f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_pred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_pred3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msBERT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"XGBOOST: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVC: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BERT: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_labels' is not defined"
     ]
    }
   ],
   "source": [
    "test_pred1 = clf.predict(test_embed).round().astype(int)\n",
    "test_pred2 = svc_model.predict(test_embed).round().astype(int)\n",
    "test_pred3 = sBERT.predict(test_input).round().astype(int)\n",
    "print(\"XGBOOST: \", accuracy_score(test_labels, test_pred1), f1_score(test_labels, test_pred1))\n",
    "print(\"SVC: \",accuracy_score(test_labels, test_pred2), f1_score(test_labels, test_pred2))\n",
    "print(\"BERT: \",accuracy_score(test_labels, test_pred3), f1_score(test_labels, test_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "\n",
    "sub['target'] = test_pred1\n",
    "sub.to_csv('submission_xgboost.csv', index=False)\n",
    "\n",
    "sub['target'] = test_pred2\n",
    "sub.to_csv('submission_svc.csv', index=False)\n",
    "\n",
    "sub['target'] = test_pred3\n",
    "sub.to_csv('submission_bertnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
